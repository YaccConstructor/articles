\section{Preliminaries}

In this section we introduce common definitions in graph theory and formal language theory which are used in this paper.
Also, we provide a brief description of Azimov's algorithm which is used as a base of our solution.

\subsection{Basic Definitions of Graph Theory}

In this paper we use a labeled directed graph as a data model and define it as follows.
\begin{definition} \emph{Labeled directed graph} is a tuple of six elements $D = (V, E, \Sigma_V, \Sigma_E, \lambda_V, \lambda_E)$, where
\begin{itemize}
    \item $V$ is a finite set of vertices. For simplicity, we assume that the vertices are natural numbers from $0$ to $|V|-1$.
    \item $E \subseteq V \times V$ is a set of edges.
    \item $\Sigma_V$ and $\Sigma_E$ are sets of labels of vertices and edges respectively, such that $\Sigma_V \cap \Sigma_E = \varnothing$.
    \item $\lambda_V : V \xrightarrow{} 2^{\Sigma_V}$ is a function that maps a vertex to a set of its labels, which can be empty.
    \item $\lambda_E : E \xrightarrow{} 2^{\Sigma_E} \setminus \{\varnothing\}$ is a function that maps an edge to a non-empty set of its labels, so each edge must have at least one label.
\end{itemize} 
\end{definition}
%~\cite{Angles2018ThePG}
Labeled graph is the basis of the widely-used \textit{property graph} data model and allows one to use not only edge labels but also vertex labels in navigation queries.

An example of the labeled directed graph $D_1$ is presented in figure~\ref{fig:example_input_graph}. Here the sets of labels $\Sigma_V = \{x, y\}$ and $\Sigma_E = \{a, b, c, d\}$.
We omit vertex labels set if it is empty.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[shorten >=1pt,auto]
       \node[elliptic state] (q_0)                        {$0:\{x, y\}$};
       \node[state] (q_1) [right=of q_0]         {$1$};
       \node[elliptic state] (q_2) [below=of q_1]    {$2: \{x\}$};
       \node[state] (q_3) [below=of q_0]    {$3$};
       \node[state] (q_5) [right=of q_1]   {$5$};
       \node[elliptic state] (q_4) [below=of q_5]   {$4: \{y\}$};
       \path[->]
        (q_0) edge  node {$\{a\}$} (q_1)
        (q_1) edge  node {$\{b\}$} (q_5)
        (q_1) edge[left]  node {$\{a, b\}$} (q_2)
        (q_3) edge[above]  node {$\{c\}$} (q_2)
        (q_4) edge[bend left]  node {$\{c\}$} (q_3)
        (q_2) edge[above]  node {$\{c\}$} (q_4)
        (q_5) edge[bend left, right]  node {$\{d\}$} (q_4)
        (q_4) edge[bend left, left]  node {$\{d\}$} (q_5);
    \end{tikzpicture}
    \caption{The input graph $D_1$}
    \label{fig:example_input_graph}
\end{figure}

\begin{definition}
Path $\pi$ in the graph $D=(V, E, \Sigma_V, \Sigma_E, \lambda_V, \lambda_E)$ is a finite sequence of vertices and edges $(v_0, e_0, v_1, e_1, ..., e_{n-1}, v_{n})$, where $\forall i, 0 \leq i \leq n: v_i \in V$; $\forall j, 1 \leq j \leq n: e_j=(v_j, v_{j+1}) \in E$.
\end{definition}

\begin{definition}
An \emph{adjacency matrix} $M$ of the graph $D$ is a matrix of size  $|V|\times|V|$, such that
\begin{equation*}
M[i,j] =
 \begin{cases}
   \lambda_E((i, j))&, (i, j) \in E\\
   \varnothing&, otherwise
 \end{cases}
\end{equation*} 
\end{definition}

Adjacency matrix $M$ of the graph $D_1$~(fig.~\ref{fig:example_input_graph}) is the following:
{
    \renewcommand{\arraystretch}{0.5}
    \setlength\arraycolsep{1.5pt}
$$
    M =
    \begin{pmatrix}
    \varnothing     & \{a\} &   \varnothing      &   \varnothing   &   \varnothing   &   \varnothing   \\
    \varnothing     &   \varnothing   & \{a, b\} &   \varnothing   &    \varnothing  & \{b\} \\
    \varnothing     &   \varnothing   &   \varnothing      &   \varnothing   & \{c\} &   \varnothing   \\
    \varnothing     &   \varnothing   & \{c\}    &   \varnothing   &   \varnothing   &   \varnothing   \\
    \varnothing     &   \varnothing   &   \varnothing      & \{c\} &   \varnothing   & \{d\} \\
    \varnothing     & \varnothing     &   \varnothing      &   \varnothing   & \{d\} &   \varnothing
    \end{pmatrix}.
$$
}

\begin{definition}
Let $M$ be an adjacency matrix of the graph $D$. Then the \emph{adjacency matrix of label} $l \in \Sigma_E$ of graph $D$ is a matrix~$\mathcal{E}^l$ of size  $|V| \times |V|$, such that
\begin{equation*}
\mathcal{E}^l[i,j] =
 \begin{cases}
   1 &, l \in M[i,j]\\
   0 &, otherwise
 \end{cases}
\end{equation*} 
\end{definition}

\begin{definition}
A \emph{boolean decomposition of adjacency matrix} $M$ of the graph $D$ is a set of Boolean matrices $\mathcal{E} = \{\mathcal{E}^l \mid l \in \Sigma_E\},$
where $\mathcal{E}^l$ is the adjacency matrix of label $l$. 
\end{definition}

For example, the boolean decomposition of the adjacency matrix $M$ of the graph $D_1$ is the set of matrices $\mathcal{E}^a, \mathcal{E}^b, \mathcal{E}^c, \mathcal{E}^d$:
{
    \renewcommand{\arraystretch}{0.5}
    \setlength\arraycolsep{1.5pt}
\begin{align*}
\mathcal{E}^a =
\begin{pmatrix}
    . & 1 & . & . & . & . \\
    . & . & 1 & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & .
\end{pmatrix},~
\mathcal{E}^b =
\begin{pmatrix}
    . & . & . & . & . & . \\
    . & . & 1 & . & . & 1 \\
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & .
\end{pmatrix},\\
\mathcal{E}^c =
\begin{pmatrix}
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & 1 & . \\
    . & . & 1 & . & . & . \\
    . & . & . & 1 & . & . \\
    . & . & . & . & . & .
\end{pmatrix},~
\mathcal{E}^d =
\begin{pmatrix}
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & 1 \\
    . & . & . & . & 1 & .
\end{pmatrix}.
\end{align*}
}

\begin{definition}

A \emph{vertex label matrix} $H$ of the graph $D$ is a matrix of size $|V|\times|V|$, such that
\begin{equation*}
H[i,j] =
  \begin{cases}
    \lambda_V (i) &, i = j \\
    \varnothing   &, otherwize
  \end{cases}
\end{equation*}
\end{definition}

The vertex label matrix $H$ of the example graph $D_1$ is
{
    \renewcommand{\arraystretch}{0.5}
    \setlength\arraycolsep{1.5pt}
$$
    H =
    \begin{pmatrix}
    \{x, y\}        & \varnothing     &   \varnothing      &   \varnothing   &   \varnothing   &    \varnothing  \\
    \varnothing     &   \varnothing   & \varnothing        &   \varnothing   & \varnothing     & \varnothing     \\
    \varnothing     &   \varnothing   &   \{x\}            &   \varnothing   & \varnothing     &   \varnothing   \\
    \varnothing     &   \varnothing   & \varnothing        &   \varnothing   &   \varnothing   &   \varnothing   \\
    \varnothing     &   \varnothing   &   \varnothing      &    \varnothing  &   \{y\}         & \varnothing     \\
    \varnothing     & \varnothing     &   \varnothing      &   \varnothing   & \varnothing     &   \varnothing
    \end{pmatrix}.
$$
}

\begin{definition}
Let $H$ be a vertex label matrix of graph $D$. Then the \emph{vertices matrix of label} $l$ is a matrix $\mathcal{V}^l$ of size $|V|\times|V|$, such that
\begin{equation*}
\mathcal{V}^l[i,j] =
  \begin{cases}
    1  &, l \in H[i, j] \\
    0  &, otherwise
  \end{cases}
\end{equation*}
\end{definition}

\begin{definition}
A \emph{boolean decomposition of vertex label matrix} $H$ of the graph $D$ is the set of Boolean matrices
$\mathcal{V} = \{V^l \mid l \in \Sigma\},$
where $\mathcal{V}^l$ is a vertices matrix of label $l$.
\end{definition}

Vertex label matrix $H$ of the graph $D_1$ can be decomposed into a set of the following Boolean matrices:
{
    \renewcommand{\arraystretch}{0.5}
    \setlength\arraycolsep{1.5pt}
\begin{align*}
\mathcal{V}^x =
\begin{pmatrix}
    1 & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & 1 & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & .
\end{pmatrix},~
\mathcal{V}^y =
\begin{pmatrix}
    1 & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & 1 & . \\
    . & . & . & . & . & .
\end{pmatrix}.
\end{align*}
}

\subsection{Basic Definitions of Formal Languages}
We use context-free grammars as paths constraints, thus in this subsection we define context-free languages and grammars.

\begin{definition}A \emph{context-free grammar} $G$ is a tuple $(N, \Sigma, P, S)$, where
\begin{itemize}
    \item $N$ is a finite set of nonterminals
    \item $\Sigma$ is a finite set of terminals, $N \cap \Sigma = \varnothing$
    \item $P$ is a finite set of productions of the form $A \to \alpha$, where $A \in N,\ \alpha \in (N \cup \Sigma)^*$
    \item $S$ is the start nonterminal
\end{itemize} 
\end{definition}

\begin{definition} A \emph{context-free language} is a language generated by a context-free grammar $G$:
%\begin{align*}
  $   L(G) = \{w \in \Sigma^* \mid S \xLongrightarrow[G]{*} w \}$
%\end{align*}
Where $S \xLongrightarrow[G]{*} w$  denotes that a string $w$ can be generated from a starting non-terminal $S$ using a sequence of productions from $P$.
\end{definition}

%\begin{definition} A context-free grammar $G = (N, \Sigma, P, S)$ is in \emph{Chomsky normal form} if every production in $P$ has one of the following forms:
%    \begin{itemize}
%        \item $A \rightarrow BC$, where $A \in N,~B,~C \in N \setminus \{S\}$
%        \item  $A \rightarrow a$, where $A \in N,~a \in \Sigma$
%        \item $S \rightarrow \varepsilon$, where $\varepsilon$ is an empty string.
%    \end{itemize}
%\end{definition}

\begin{definition} A context-free grammar $G = (N, \Sigma, P, S)$ is in \emph{weak Chomsky normal form} (WCNF) if every production in $P$ has one of the following forms:
    \begin{itemize}
        \item $A \rightarrow BC$, where $A, B, C \in N$
        \item  $A \rightarrow a$, where $A \in N, a \in \Sigma$
        \item $A \rightarrow \varepsilon,~A \in N$
    \end{itemize}
\end{definition}

In other words, weak Chomsky normal form differs from Chomsky normal form in the following:
\begin{itemize}
    \item $\varepsilon$ can be derived from any non-terminal;
    \item $S$ can occur in the right-hand side of productions.
\end{itemize}

The matrix-based CFPQ algorithms process grammars only in weak Chomsky normal form, but every context-free grammar can be transformed into the equivalent grammar in this form.

Consider the context-free grammar $G_1=(\{S\},\{c, d, y\}, P, S)$, where $P$ contains two rules:
%\begin{align}
%\label{eqn:g1_example}
$S \rightarrow c \ S \ d; \ \ \ 
S \rightarrow c \ y \ d
$.%\end{align}

This grammar generates the context-free language: $$L(G_1) = \{c^nyd^n, n \in \mathbb{N}\}.$$
The following grammar $G_1^{\text{wcnf}}$ is a result of the transformation of $G_1$ to weak Chomsky normal form:
\begin{align*}
S& \to C \ E   & E& \to Y \ D   & C& \to c &D& \to d   \\
S& \to C \ S_1 & S_1& \to S \ D & Y& \to y &&  \\
\end{align*}


\subsection{Context-Free Path Querying}

\begin{definition}
Let $D = (V, E, \Sigma_V, \Sigma_E, \lambda_V, \lambda_E)$ be a labeled graph, $G = (N, \Sigma_V \cup \Sigma_E, P, S)$ be a context free grammar. Then a \emph{context free relation} with grammar $G$ on the labeled graph $D$ is the relation $R_{G, D} \subseteq V \times V$:
\begin{equation*} \label{eq1}
\begin{split}
R_{G, D} = \{(v_1, v_n) \in V \times V  \mid \ &\exists \pi = (v_1, e_1, v_2, \ldots, e_n, v_n) \in \pi(D): \\
      &\ l(\pi) \cap L(G) \neq \varnothing \},
\end{split}
\end{equation*}
where $l(\pi) \subset (\Sigma_V \cup \Sigma_E)^*$ is the set of labels along the path $\pi$:
$$l(\pi) = \lambda_V(v_1)^* \cdot \lambda_E(e_1) \cdot \lambda_V(v_2)^* \cdot \lambda_E(e_2) \cdot \ldots \cdot \lambda_E(e_n) \cdot \lambda_V(v_n)^*$$
\
\end{definition}

For example, $\pi$ is a path from vertex 2 to vertex 5 in the labeled graph presented in figure~\ref{fig:example_input_graph}:
$\pi=2:\{x\}\xrightarrow{\{c\}} 4:\{y\} \xrightarrow{\{d\}} 5.$

Labels along $\pi$ form the set of sequences $l(\pi) = \{x^mcy^nd \mid n \geq 0, m \geq 0\}$.
Only one of these sequences satisfies context-free constraints of the grammar $G_1$: $cyd$.
Hence $l(\pi) \cap L(G_1) \neq \varnothing$ and the pair $(3,6) \in R_{G_1, D}$.

Take a closer look at the definition of path labels, namely that it allows for zero or more repetitions of a label of each vertex.
This makes it possible to omit vertex labels or, if there are many vertex labels, to use them in an arbitrary order.
It also permits to write a query which uses one vertex label multiple times.
This definition may appear strange in some cases, but it depends on the semantics of the graph query language.
Semantics formalization is a future work, so we will stick to this definition in this paper.

Finally, we can define context-free path querying problem.
\begin{definition}
    \emph{Context-free path querying problem} is the problem of finding context-free relation $R_{G, D}$ for a given directed labeled graph $D$ and a context-free grammar $G$.
\end{definition}

In other words, the result of context-free path query evaluation is a set of vertex pairs such that there is a path between them and this path forms a word from the given language.

The context-free relation $R_{G_1,D_1}$ for the graph $D_1$ and the context-free free grammar $G_1$ is the following:
$$R_{G_1, D_1} = \{(2, 4), (2, 5), (3, 4), (3, 5), (4, 4), (4, 5)\}.$$
Note that any relation $R_{G, D}$ can be represented as a Boolean matrix: $T[i,j] = 1 \iff (i,j) \in R_{G, D}.$
In our example, $R_{G_1, D_1}$ can be represented as follows:
{
    \renewcommand{\arraystretch}{0.5}
    \setlength\arraycolsep{1.5pt}
\begin{align*}
T =
\begin{pmatrix}
    . & . & . & . & . & . \\
    . & . & . & . & . & . \\
    . & . & . & . & 1 & 1 \\
    . & . & . & . & 1 & 1 \\
    . & . & . & . & 1 & 1 \\
    . & . & . & . & . & .
\end{pmatrix}.
\end{align*}
}

\begin{definition}
    Suppose $Src$ is a given set of start vertices, then \textit{multiple-source context-free path querying problem} for the given $Src$, directed labeled graph $D$ and context-free grammar $G$ is to find a context-free relation
    $R_{G, D}^{Src} \subseteq Src\times V \subseteq R_{G,D}.$
    Thus we restrict start vertices of the paths of interest to be a vertices from the given set
\end{definition}

As a special case, a \emph{single-source CFPQ} is when $Src$ is a singleton set.
If we set $Src=\{2\}$ in the previous example, then the result is: $R_{G_1, D_1}^{\{2\}} = \{(2, 4), (2, 5)\}.$
%We can represent the $R_{G_1, D_1}^{\{2\}}$ as a Boolean matrix:
%{
%    \renewcommand{\arraystretch}{0.7}
%    \setlength\arraycolsep{2pt}
%\begin{align*}
%T =
%\begin{pmatrix}
%    . & . & . & . & . & . \\
%    . & . & . & . & . & . \\
%    . & . & . & . & 1 & 1 \\
%    . & . & . & . & . & . \\
%    . & . & . & . & . & . \\
%    . & . & . & . & . & .
%\end{pmatrix}.
%\end{align*}
%}

\subsection{Matrix-Based Algorithm}
Our algorithm is based on the Azimov's CFPQ algorithm~\cite{Azimov:2018:CPQ:3210259.3210264} which is based on matrix operations.
This algorithm reduce CFPQ to operations over Boolean matrices and as a result allows one to use high-performance linear algebra libraries and utilize modern parallel hardware for CFPQ.
Moreover, utilization of Boolean matrices simplifies the implementation of the algorithm.

%{
%\begin{algorithm}
%\begin{algorithmic}[1]
%\caption{Context-free path querying algorithm}
%\label{alg:algo0}
%\Function{evalCFPQ}{$D=(V,E, \Sigma_V, \Sigma_E, \lambda_V, \lambda_E)$,\par
%\hskip\algorithmicindent $G=(N,\Sigma,P,S)$}
%    \State{$n \gets$ |V|}
%    \State{$T \gets \{T^{A_i} \mid A_i \in N, T^{A_i}[i,j] \gets \textit{false}$, for all $i,j$\} }
%    \ForAll{$(i,j) \in E$, $A_k \mid \lambda_E(i,j) = x, A_k \to x \in P$}
%        %\Comment{Matrices initialization}
%        %\For{$A_k \mid A_k \to x \in P$}
%          {$T^{A_k}_{i,j} \gets \texttt{true}$}
%        %\EndFor
%    \EndFor
%    \ForAll{$A_k \mid A_k \to \varepsilon \in P$}
%        \ForAll{$i \in \{0,\ldots ,n-1\}$}
%            {$T^{A_k}[i,i] \gets \textit{true}$}
%        \EndFor
%    \EndFor
%
%    \While{any matrix in $T$ is changing}
%        %\Comment{Transitive c	losure calculation}
%        \For{$A_i \to A_j A_k \in P$}
%          { $T^{A_i} \gets T^{A_i} + (T^{A_j} * T^{A_k})$ }
%        \EndFor
%    \EndWhile
%\State \Return $T$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%}

Note, that the algorithm computes not only the context-free relation $R_{G,D}$ but a set of context-free relations $R_{A,D} \subseteq V \times V$ for every $A \in N$.
Thus it provides information about paths which form words derivable from any nonterminal in the given grammar.
Also, this algorithm handles only the edge labels.

As was shown by Nikita Mishin et al.~\cite{Mishin:2019:ECP:3327964.3328503} and Arseniy Terekhov et al.~\cite{ 10.1145/3398682.3399163}, this algorithm can be implemented using various high-performance programming techniques (including GPGPU utilization), and it is applicable for real-world graph analysis.
But this algorithm solves \textit{all-pairs} version of CFPQ: it finds all pairs of vertices in the given graph, such that there exist a path between them which forms a word in the given language.
Thus it is impractical in cases when we are only interested in paths which start from the specific set of vertices, especially if this set is relatively small.
Moreover, Azimov's algorithm operates over an adjacency matrix of the whole input graph, and as a result it requires a huge amount of memory, which may be a problem for a real-world graph database.

