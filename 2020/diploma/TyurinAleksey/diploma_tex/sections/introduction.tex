\section*{Introduction}\label{sec:introduction}
In the era of big data and high load computations, graphical processing units (GPUs) are used extensively for data processing. There have even been designed embedded devices with the support of GPUs for general purpose computations, like image recognition on mobile robots~\cite{NVJETSON}. However, since many GPU-based applications tend to be bandwidth bound, meaning that data allocation and access are the main bottlenecks, memory optimizations appear to be in a prevailing significance and are addressed in a huge amount of research.

While the problem of available GPU memory could be tackled with sophisticated memory pooling techniques through memory swapping and sharing~\cite{zhang2019efficient}, memory architecture of a GPU often requires more sophisticated optimizations. Given that a typical GPU incorporates several memory types, each having different capacity and throughput, a few memory management automation techniques have been introduced. They could leverage more effective memory spaces to handle register spilling and systematically consider the performance benefit achievable through a specific allocation of shared memory to save global memory transactions~\cite{AutomaticSharedMem,RegisterSpilling}. Further, the diversity of memory types imposes that each memory access should satisfy memory type specific patterns, to be most effective. Under the non-fulfillment of these patterns the data throughput of an application is aggravated due to the increase in the number of required memory transactions. Considering the following typical scenario of a GPU-accelerated application, another runtime memory optimization could be proposed.

 To facilitate the data processing a GPU routine is executed by multiple threads simultaneously with different pieces of data, often exceeding the maximal number of threads that could be executed simultaneously resulting in blocks of threads being executed iteratively.
 Being that the input data often exceeds the available GPU memory, the routine could not be applied at once, and there is a need to split the data into chunks and process them iteratively by the routine.
 It happens that some relatively small properties within a processing routine are maintained between the iterations and could be considered static in that sense.
 For example, if the application is a GPU-accelerated data processing engine that allows one to write queries to data and typically these queries remain small if compared to data and take significant execution time, the query, once specified by the user, can be used as a static, i.e. already known and constant, data for the query execution kernel runtime optimization since it remains unchanged during the host code execution.
 This observation allows to apply a \textit{partial evaluation} technique to optimize such routines in runtime.

\textit{Partial evaluation} or program \textit{specialization}~\cite{Jones1993,PartialEvalPaper} is a program transformation and optimization technique that optimizes a given program with respect to statically known inputs, producing another program which, if given only the remaining dynamic inputs, will produce the same results as initial one would have produced, given both inputs.
Basically, a partial evaluator performs an aggressive unfolding/unrolling, inlining, and constant propagation.  
The application of partial evaluation at runtime has recently shown a significant performance improvement of query execution for CPU-based database querying~\cite{LLVM-mix}. 

Regarding memory optimizations of GPU kernels, partial evaluation is able to embed static data memory accesses into the code, i.e. place it directly into registers, once a kernel is properly written, which could result in a better performance compared to non-embedded access since memory transactions would be replaced by accesses to the instruction cache. Thus, the aim of this work is to provide an empirical evaluation of an existing partial evaluator AnyDSL~\cite{LeiBa} that is capable of producing CUDA~\footnote{Programming and hardware model by NVIDIA.} code for NVIDIA GPUs to investigate the effects that appear after partially evaluating GPU applications and what aspects of GPU architecture affect the desired result and whether any significant performance improvements could be achieved at all.