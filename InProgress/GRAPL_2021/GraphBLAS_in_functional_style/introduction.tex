\section{Introduction}

One of the promising ways to high-performance graph analysis is based on the utilization of linear algebra: operations over vectors and matrices can be efficiently implemented on modern parallel hardware, and once we reduce the given graph analysis problem to the composition of such operations, we get a high-performance solution for our problem. 
A well-known example of such reduction is a reduction of all-pairs shortest path (APSP) problem to matrix multiplication over appropriate \textit{semiring}.
GraphBLAS API standard~\cite{7761646} provides formalization and generalization of this observation and make it useful in practice. 
GraphBLAS API introduces appropriate algebraic structures (monoid, semiring), objects (scalar, vector, matrix), and operations over them to provides building blocks to create graph analysis algorithms.
It was shown, that sparse linear algebra over specific semirings is useful not only for graph analysis, but also in other areas, such as computational biology~\cite{10.5555/3433701.3433800} and machine learning~\cite{8091098}.

There are a number of GraphBLAS API implementations, such as SuiteSarse:GraphBLAS~\cite{10.1145/3322125} and CombBLAS~\cite{10.1177/1094342011403516}, but all of them do not utilize the power of GPGPU, except GraphBLAST~\cite{10.1145/3466795}, while GPGPU utilization for linear algebra is a common practice today. 
GPGPU development is difficult itself because it introduces heterogeneous computational device, special programming model, and specific optimizations.
Implementation of GraphBLAS API even more challenging, because it means the processing of irregular data, and the creation of generic (polymorphic) functions to declare and use user-defined semirings which is hard to express in low-level programming languages like CUDA C or OpenCL C which are usually used for GPGPU programming.
Moreover, it is necessary to use high-level optimizations, like kernel fusion or elimination of unnecessary computations to improve the performance of end-user solutions based on the provided API implementation.
But such high-level optimizations are too hard to automate for C-like languages.

Functional programming can help to solves these problems.
First of all, native support functions as parameters simplify semirings descriptions and implementation of functions parametrized with semirings. 
Moreover, a powerful type system allows one to describe abstract (generic) functions which simplifies the development and usage of abstract linear algebra operations. 
Even more, such native features of functional programming languages, like discriminated unions (union types) and strong static typing allows one to create more robust code.
For example, discriminated unions allows one naturally express \texttt{Min-Plus} semiring, where we should equip $\mathbb{R}$ with special element $\infty$ (infinity, namely identity element for $\oplus$), so we cannot use predefined types like \texttt{float} or \texttt{double}.
Another area where functional programming can be useful is automatic code optimization.
A big number of nontrivial optimizations for functional languages for GPGPU were developed, such as specialization, deforestation, and kernels fusion, one of the actively discussed optimizations in GraphBLAS community~\cite{10.1145/3466795}.
These techniques make programs in high-level programming languages competitive in terms of performance with solutions written in CUDA or OpenCL C. 
For more details one can look at such languages and frameworks as Futhark\footnote{Futhark is a purely functional statically typed programming language for GPGPU. Project web page: \url{https://futhark-lang.org/}. Access date: 12.01.2021.}~\cite{Henriksen:2017:FPF:3062341.3062354}, Accelerate\footnote{Accelerate: GPGPU programming with Haskell. Project web page:\url{https://www.acceleratehs.org/}. Access date: 12.01.2021.}~\cite{10.1145/2544174.2500595}, AnyDSL\footnote{AnyDSL is a partial evaluation framework for parallel programming. Project web page: \url{https://anydsl.github.io/}. Access date: 12.01.2021.}~\cite{10.1145/3276489}.

In this work we discuss a way to implement GraphBLAS API which combines high-performance computations on GPGPU and the power of high-level programming languages in both application development and possible code optimizations.
Our solution is based on metaprogramming techniques: we propose to generate code for GPGPU from a high-level programming language.
Namely, we plan to generate OpenCL C from a subset of  F\# programming language.
To translate F\# to OpenCL C we use a Brahma.FSharp\footnote{Brahma.FSharp project on GitHub: \url{https://github.com/YaccConstructor/Brahma.FSharp}. Access date: 12.01.2021.} which is based on F\# quotations metaprogramming techniques\footnote{F\# code quotations is a run time metaprogramming technique which allows one to transform written F\# code during program execution. Official documentation: \url{https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/code-quotations}. Access date: 12.01.2021.}.
Usage of F\# simplifies both implementation of GraphBLAS API, making features of functional programming available, and its utilization in application development with high-level programming language on .NET platform. 
Moreover, as far as F\# is a functional-first programming language, it should make it possible to use advanced optimization techniques and power of type system.
The choice of OpenCL C as a target language provides high portability: it is possible to run OpenCL C code on multi-thread CPU, on different GPGPUs (not only Nvidia), and even on FPGA~\cite{kenter2019invited, 6567546}.
%The utilization of FPGAs may open a way to hardware acceleration of sparse linear algebra and, as a result, of many solutions in different areas such as graph analysis, computational biology, machine learning.


To summarize, in this work
\begin{enumerate}
    \item We show how types can naturally solve zeroes problem   !!!
    \item We show that utilization of generic high-order functions can simplify API by kernels unification
    \item We evaluate matrix-matrix element-wise functions to show that the proposed way !!! GPGPUs dddd 
\end{enumerate}   