\section{Introduction}

One technique to efficiently solve a data analysis problem is to formulate it in terms of operations over vectors and matrices (in terms of linear algebra).
This way it is possible to employ a set of reliable mathematical tools and solutions.
Another advantage of this approach is the ability to evaluate the problem with \textit{zero-cost} by high-performance linear algebra libraries, which utilize modern hardware, provide various optimization techniques, and allow one to prototype a solution in code with predefined building blocks quickly and safely.
GraphBLAS API\footnote{GraphBLAS project web page: \url{https://graphblas.github.io/}. Access date: 19.01.2021.}~\cite{7761646} is one of the standards that introduce such building blocks.
GraphBLAS takes into account the sparsity of data by using sparse formats of matrices and vectors, and generalizes the building blocks by operating with arbitrary \textit{monoids} and \textit{semirings}.
While initially GraphBLAS was focused on graph analysis, it was shown that the proposed approach can be successfully used for data analysis in other areas, such as computational biology~\cite{10.5555/3433701.3433800} and machine learning~\cite{8091098}.

GPGPU utilization for data analysis and for linear algebra operations is a promising way to high-performance data analysis because GPGPU is much more powerful in parallel data processing.
Unfortunately, GPGPU programming is very challenging.
It introduces heterogeneous device model into the system, memory traffic, and data operations limitations, as well as requires taking into account vendor-specific capabilities.
Best to our knowledge, there is no complete implementation of GraphBLAS API on GPGPU, except for the GraphBLAST project\footnote{GraphBLAST project: \url{https://github.com/gunrock/graphblast}. Access date: 19.01.2021.}~\cite{yang2019graphblast}, which is currently in active development.

The sparsity of data introduces issues with load balancing, irregular data access, thus sparsity complicates the implementation of high-performance algorithms for sparse linear algebra on GPGPU even more.
As a result, there is a huge number of different formats for sparse matrices and vectors representation, such as CSR, COO, Quad-tree, and a huge number of algorithms for operations over these formats.
See~\cite{Gao2020ASS} for a significant survey of sparse matrix-matrix multiplication algorithms.
Algorithms for different operations, such as matrix-matrix multiplication and matrix-vector multiplication are developed independently.
Thus, there are no sparse linear algebra libraries based on the state-of-the-art algorithms.
Moreover, existing libraries, such as cuSparse\footnote{NVIDIA sparse matrix library (in Cuda) \url{https://docs.nvidia.com/cuda/cusparse/}. Access date: 19.01.2021.}, clSparse\footnote{Sparse linear library functions in OpenCL: \url{http://clmathlibraries.github.io/clSPARSE/}. Access date: 19.01.2021.}~\cite{10.1145/2909437.2909442}, or more modern CUSP\footnote{CUSP sparse linear algebra library: \url{https://cusplibrary.github.io/modules.html}. Access date: 19.01.2021.} or bhSparse\footnote{bhSparse sparse matrix multiplication library: \url{https://github.com/weifengliu-ssslab/bhSPARSE}. Access date: 19.01.2021.}~\cite{10.1016/j.jpdc.2015.06.010}, are focused on numerical computations over floats or doubles, not on generic data processing over arbitrary semirings which is required for GraphBLAS API implementation.

An important partial case of linear algebra is the sparse Boolean linear algebra.
Boolean algebra is suitable for problems over a finite set of values, such as transitive closure of a relation or a graph, regular and context-free path queries for graphs~\cite{10.1145/3210259.3210264}, as well as parsing for different classes of languages, such as Context-Free~\cite{10.1016/S0022-0000(75)80046-8}, Boolean and Conjunctive~\cite{OKHOTIN2014101}, Multiple Context-Free(MCFL)~\cite{10.5555/972525.972527}.
Moreover, some operations over the Boolean semiring can be used as building blocks for algorithms over other semirings.
\cho{For example, to compute the shape of the result of the operation.}
Thus, sparse Boolean linear algebra is an important partial case both as a way to solve applied problems and as a building block for other algorithms.
\cho{However, sparse Boolean linear algebra on GPGPU is still not presented, because of its high specificity.}

In this paper, we present the implementation of sparse boolean linear algebra operations as a stand-alone self-sufficient programming library for the two most popular GPGPU platforms: NVIDIA Cuda\footnote{CUDA is a platform and programming model for NVIDIA devices. Home page: \url{https://developer.nvidia.com/CUDA-zone}. Access date: 19.01.2021.}and OpenCL\footnote{OpenCL is an open standard for parallel programming of heterogeneous systems. Home page: \url{https://www.khronos.org/opencl/}. Access date: 19.01.2021.}.
Cuda is a GPGPU technology for NVIDIA devices which employs some platform-specific facilities, such as unified memory mechanism, and make architectural assumptions which gives more optimizations space at the cost of portability.
OpenCL is a platform-agnostic API standard, which allows for running computations on different platforms, such as multi-threaded CPUs, GPUs, and FPGAs.
Our implementation relies on modern techniques of sparse matrices processing and exploits some optimizations, related to the boolean data processing.
Moreover, we provide a Python API to simplify utilization of our library.
Preliminary evaluation shows that such operation as matrix-matrix multiplication specialized for Boolean matrices can be up to 2 times faster and consume up to YYY times less memory in comparison with generic operations from such libraries as CUSP or CuSparse.