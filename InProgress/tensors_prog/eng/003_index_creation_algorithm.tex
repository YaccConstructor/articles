\subsection{Index Creation Algorithm}

The \textit{index creation} algorithm outputs the final adjacency matrix for the input graph with all pairs of vertices which are reachable through some nonterminal in the input grammar $G$, as well as the index matrix which is to be used to extract paths in the \textit{path extraction} algorithm.

The algorithm is based on the generalization of the FSM intersection for an RSM,  and the edge-labeled directed input graph.
Since the RSM is composed as a set of FSMs, it could easily be presented as an adjacency matrix for some graph over the set of labels.
As shown in the Def.~\ref{def:bool:product}, we can apply Kronecker product for Boolean matrices to \textit{intersect} the RSM and the input graph to some extent.
But the RSM contains nonterminal symbols with the additional logic of \textit{recursive calls}, which requires a \textit{transitive closure} step to extract such symbols.

The core idea of the algorithm comes from the Kronecker product and transitive closure.
The algorithm boils down to the evaluation of the iterative Kronecker product for the adjacency matrix $\mathcal{M}_1$ of the RSM $R$ and the adjacency matrix $\mathcal{M}_2$ of the  input graph $\mathcal{G}$, followed by the transitive closure, extraction of nonterminals and updating the graph adjacency matrix $\mathcal{M}_2$.
Listing~\ref{tensor:cfpq:cubic} demonstrates the main steps of the algorithm.
\begin{algorithm}[h]
\floatname{algorithm}{Listing}
\begin{algorithmic}[1]
\footnotesize
\caption{Kronecker product based CFPQ using dynamic transitive closure}
\label{tensor:cfpq:cubic}
\Function{contextFreePathQuerying}{G, $\mathcal{G}$}
    % Input data preparation
   \State{$n \gets$ the number of nodes in $\mathcal{G}$}
    \State{$R \gets$ recursive automata for $G$}
    \State{$\mathcal{M}_1 \gets$ the set of adjacency matrices for $R$}
    \State{$\mathcal{M}_2, \mathcal{A}_2 \gets$ the sets of adjacency matrices for $\mathcal{G}$}
    %\State{$M_3 \gets$ The empty matrix}
    \State{$C_3, M_3 \gets$ the empty matrices of size $dim(\mathcal{M}_1)n \times dim(\mathcal{M}_1)n$}
    % Eps-transition handling for graph
    \For{$s \in 0..dim(\mathcal{M}_1)-1$}
        \For{$S \in \textit{getNonterminals}(R,s,s)$}
            \For{$i \in 0..dim(\mathcal{M}_2)-1$}
                \State{$M^S_2[i,i] \gets 1 $}
            \EndFor
        \EndFor
    \EndFor
    \While{$\mathcal{M}_2$ is changing}
        \State{$M_3' \gets \bigvee_{M^S \in \mathcal{M}_1 \otimes \mathcal{A}_2} M^S$}
        \Comment{Using only new edges from $\mathcal{A}_2$}
        \State{$M_3 \gets M_3 + M_3'$}
        \Comment{Updating the matrix for the Kronecker product result}
        %\State{$\mathcal{A}_2 \gets$ The empty matrix of size $n \times n$}
        \State{$\mathcal{A}_2 \gets$ the empty matrix}
        \State{$C_3' \gets$ the empty matrix of size $dim(\mathcal{M}_1)n \times dim(\mathcal{M}_1)n$}
        %\Comment{Evaluate Kroncker product}
        \For{$(i,j) \mid M_3'[i,j] \neq 0$}
            %\State{$C_3'[i,j] \gets 1$}
            \State{$C_3' \gets \textit{add}(C_3, C_3', i, j)$}
            \Comment{Updating the transitive closure}
            \State{$C_3 \gets C_3 + C_3'$}
        \EndFor
        %\State{$n \gets$ dim($M_3)$}
        %\Comment{Matrix $M_3$ size = $n \times n$}
        % Add non-terminals (possibly new)
        \For{$(i,j)\ |\ C_3'[i,j] \neq 0$}
                \State{$s, f \gets \textit{getStates}(C_3',i,j)$}
                \State{$x, y \gets \textit{getCoordinates}(C_3',i,j)$}
                \For{$S \in \textit{getNonterminals}(R,s,f)$}
                    \State{$M^S_2[x,y] \gets 1$}
                    \State{$A^S_2[x,y] \gets 1$}
                \EndFor
        \EndFor
    \EndWhile
\State \Return $\mathcal{M}_2, M_3$
\EndFunction
% \Function{add}{$C, C', i, j$}
%     \State{$C'[i,j] \gets {1}$}
%     \For{$(u,v) \mid C[u,i] = C[j,v] = 1, C[u,j] = C[u,v] = 0$}
%         \State{$C'[u,v] \gets {1}$}
%     \EndFor
%     \State \Return{$C'$}
% \EndFunction
\Function{getStates}{$C, i, j$}
    \State{$n \gets dim(\mathcal{M}_2)$}
   \Comment{$\mathcal{M}_2$ the set of adjacency matrices for $\mathcal{G}$}
    \State \Return{$\left\lfloor{i / n}\right\rfloor, \left\lfloor{j / n}\right\rfloor$}
\EndFunction
\Function{getCoordinates}{$C, i, j$}
    \State{$n \gets dim(\mathcal{M}_2)$}
    \State \Return{$i \bmod n, j \bmod n$}
\EndFunction
\end{algorithmic}
\end{algorithm}
\subsubsection{Application of Dynamic Transitive Closure}
The most time-consuming steps of the algorithm are the computations of the Kronecker product and transitive closure.
Note that the adjacency matrix $\mathcal{M}_2$ is changed incrementally i.e. elements (edges) are added to $\mathcal{M}_2$ at each iteration of the algorithm and are never deleted from it.
So it is not necessary to recompute the whole product or transitive closure if some appropriate data structure is maintained.

To compute the Kronecker product, we employ the fact that it is left-distributive.
Let $\mathcal{A}_2$ be a matrix with newly added elements and $\mathcal{B}_2$ be a matrix with all previously found elements, such that $\mathcal{M}_2 = \mathcal{A}_2 + \mathcal{B}_2$.
Then by left-distributivity of the Kronecker product we have $\mathcal{M}_1 \otimes \mathcal{M}_2 = \mathcal{M}_1 \otimes (\mathcal{A}_2 + \mathcal{B}_2) = \mathcal{M}_1\otimes \mathcal{A}_2 + \mathcal{M}_1 \otimes \mathcal{B}_2$.
Note that $\mathcal{M}_1 \otimes \mathcal{B}_2$ is known and is already in the matrix $\mathcal{M}_3$ and its transitive closure is also already in the matrix $C_3$, because it has been calculated at the previous iterations, so it is left to update some elements of $\mathcal{M}_3$ by computing $\mathcal{M}_1\otimes \mathcal{A}_2$.


The fast computation of transitive closure can be obtained by using an incremental dynamic transitive closure technique.
Let us describe the function $add$ from Listing \ref{tensor:cfpq:cubic}.
Let $C_3$ be a transitive closure matrix of the graph $G$ with $n$ vertices.
We use an approach by~\cite{IBARAKI198395} to maintain dynamic transitive closure.
The key idea of their algorithm is to recalculate reachability information only for those vertices which become reachable after insertion of a certain edge.

We have modified the algorithm to achieve a logarithmic speed-up in the following way.
For each newly inserted edge $(i, j)$ and every node $u \neq j$ of $G$ such that $C_3[u, i] = 1$ and $C_3[u, j]=0$, one needs to perform operation $C_3[u,v] = C_3[u, v] \wedge C_3[j, v]$ for every node $v$, where $1 \wedge 1 = 0 \wedge 0 = 1 \wedge 0 = 0$ and $0 \wedge 1 = 1$.
Notice that these operations are equivalent to the element-wise (Hadamard) product of two vectors of size $n$, where multiplication operation is denoted as $\wedge$. To check whether $C_3[u, i] = 1$ and $C_3[u, j]=0$ one needs to multiply two vectors: the first vector represents reachability of the given vertex $i$ from other vertices $\{u_1, u_2, ..., u_n\}$ of the graph and the second vector represents the same for the given vertex $j$. The operation $C_3[u, v] \wedge C_3[j, v]$ also can be reduced to the computation of the Hadamard product of two vectors of size $n$ for the given $u_k$. The first vector contains the information whether vertices  $\{v_1, v_2, ..., v_n\}$ of the graph are reachable from the given vertex $u_k$ and the second vector represents the same for the given vertex $j$. The element-wise product of two vectors can be calculated naively in time $O(n)$. Thus, the time complexity of the transitive closure can be reduced by speeding up the element-wise product of two vectors of size $n$.


To achieve logarithmic speed-up, we use the Four Russians' trick by \cite{arlazarov1970economical}.
Let us assume an architecture with word size $w= \theta(\log n)$.
First we split each vector into $n/\log n$ parts of size $\log n$.
Then we create a table $T$ such that $T(a, b)$ = $a \wedge b$ where $a, b \ \in {\{0,1\}}^{\log n}$.
This takes time $O(n^2 \log n)$, since there are $2^{\log n} = n$ variants of Boolean vectors of size $\log n$ and hence $n^2$ possible pairs of vectors $(a, b)$ in total, and each component takes $O(\log n)$ time.
Assuming constant-time logical operations on words, we can store a polynomial number of lookup tables (arrays) $T_i$ (one array for each vector of size $\log n$), such that given an index of a table $T_i$, and any $O(\log n)$ bit vector $b$, we can look up $T_i(b)$ in constant time. The index of each array $T_a$ is stored in array $T$, which can be accessed in constant time for a given $\log$-size vector $a$. Thus, we can calculate the product of two parts $a$ and $b$ of size $\log n$ in constant time using the table $T$.
There are $n/\log n$ such parts, so the element-wise product of two vectors of size $n$ can be calculated in time $O(n/\log n)$ with $O(n^2 \log n)$ preprocessing.

\begin{theorem}
    Let $\mathcal{G} =  \langle V,E,L\rangle$ be a graph and $G = \langle\Sigma, N, S, P\rangle$ be a grammar.
    Let $\mathcal{M}_{2}$ be a resulting adjacency matrix after the execution of the algorithm in Listing~\ref{tensor:cfpq:cubic}. Then for any valid indices $i, j$ and for each nonterminal $N_i \in N$ the following statement holds: the cell $M_{2,(k)}^{N_i}[i,j]$ contains $\{1\}$, iff there is a path $i\pi j$ in the graph $\mathcal{G}$ such that $ N_i \xrightarrow{*} l(\pi)$.
\end{theorem}{}
\begin{proof}
    By induction on the height of the derivation tree obtained on each iteration.
\end{proof}{}

\begin{theorem}{}
\label{theorem: subcubic}
    Let $\mathcal{G} = \langle V,E,L \rangle$ be a graph and $G = \langle\Sigma, N, S, P\rangle$ be a grammar.
    The algorithm from Listing~\ref{tensor:cfpq:cubic} calculates resulting matrices $\mathcal{M}_2$ and $M_3$ in $O({|P|}^3n^3/\log (|P|n))$ time on a word RAM with word size $w= \theta(\log |P|n)$, where $n = |V|$. Moreover, maintaining of the dynamic transitive closure dominates the cost of the algorithm.
\end{theorem}

\begin{proof}
 Let $|\mathcal{A}|$ be the number of non-zero elements in a matrix $\mathcal{A}$. Consider the total time which is needed for computing the Kronecker products. The elements of the matrices $\mathcal{A}_2^{(i)}$ are pairwise distinct on every $i$-th iteration of the algorithm therefore the total number of operations is $\sum\limits_i{T(\mathcal{M}_1 \otimes \mathcal{A}_2^{(i)})} = |\mathcal{M}_1| \sum\limits_i {|\mathcal{A}_2^{(i)}|} = (|N| + |\Sigma|){|P|}^2 \sum\limits_i {|\mathcal{A}_2^{(i)}|} = O({(|N| + |\Sigma|)}^2{|P|}^2 n^2).$


Now we derive the time complexity of maintaining the dynamic transitive closure.
Notice that $C_3$ has a size of the Kronecker product of $\mathcal{M}_1 \otimes \mathcal{M}_2$, which is equal to $dim(\mathcal{M}_1)n \times dim(\mathcal{M}_1)n = |P|n \times |P|n$ so no more than ${|P|}^2n^2$ edges will be added during all iterations of the Algorithm.
Checking whether $C_3[u, i] = 1$ and $C_3[u, j]=0$ for every node $u \in V$ for each newly inserted edge $(i, j)$ requires one multiplication of vectors per insertion, thus total time is $O({|P|}^3n^3/\log (|P|n))$.
Note that after checking the condition, at least one element $C[u', j]$ changes value from 0 to 1 and then never becomes 0 for some $u'$ and $j$.
Therefore the operation $C_3[u',v] = C_3[u', v] \wedge C_3[j, v]$ for all $v \in V$ is executed at most once for every pair of vertices $(u',j)$ during the entire computation implying that the total time is equal to $O({|P|}^2n^2|P|n/\log (|P|n))=O({|P|}^3n^3/\log (|P|n))$, using the  multiplication of vectors.


The matrix $C_3'$ contains only new elements, therefore $C_3$ can be updated directly using only $|C_3'|$ operations and hence ${|P|}^2n^2$ operations in total.
The same holds for the loop in line 19 of the algorithm from Listing~\ref{tensor:cfpq:cubic}, because operations are performed only for non-zero elements of the matrix $|C_3'|$.
Finally, the time complexity of the algorithm is $O({(|N| + |\Sigma|)}^2{|P|}^2 n^2) + O({|P|}^2n^2) + O({|P|}^2n^2 \log (|P|n)) + O({|P|}^3n^3/\log (|P|n)) + O({|P|}^2n^2)= O({|P|}^3n^3/\log (|P|n))$. \qed
\end{proof}

The complexity analysis of the Algorithm~\ref{tensor:cfpq:cubic} shows that the maintaining of the incremental transitive closure dominates the cost of the algorithm. Thus, CFPQ can be solved in truly subcubic $O(n^{3-\varepsilon})$ time if there exists an incremental dynamic algorithm for the transitive closure for a graph with $n$ vertices with preprocessing time $O(n^{3-\varepsilon})$ and total update time $O(n^{3-\varepsilon})$. Unfortunately, such an algorithm is unlikely to exist: it was proven by~\cite{10.1145/2746539.2746609} that there is no incremental dynamic transitive closure algorithm for a graph with $n$ vertices and at most $m$ edges with preprocessing time $poly(m)$, total update time $mn^{1-\varepsilon}$, and query time $m^{\delta-\varepsilon}$ for any $\delta \in (0, 1/2]$ per query that has an error probability of at most 1/3 assuming the widely believed Online Boolean Matrix-Vector Multiplication (OMv) Conjecture. OMv Conjecture introduced by~\cite{10.1145/2746539.2746609} states that for any constant $ \varepsilon>0$, there is no $O(n^{3-\varepsilon})$-time algorithm that solves OMv with an error probability of at most 1/3.

% In this section, we introduce the algorithm for context-free path querying.
%The algorithm determines the existence of a path, which forms a sentence of the language defined by
%the input RSM $R$, between each pair of vertices in the directed edge-labeled graph $\mathcal{G}$.
%The algorithm is based on the generalization of the FSM intersection for an RSM,
%and an input graph. Since a graph can be interpreted as a FSM, in which
%transitions correspond to the labeled edges between vertices of the graph,
%and an RSM is composed of a set of FSMs, the intersection of such machines
%can be computed using the classic algorithm for FSM intersection, presented
%in~\cite{automata:theory:10.5555/1177300}.
%Such a way of generalization leads to zero-overhead algorithm for RPQ, contrary to other algorithms which require regular expression to context-free grammar transformation.

%The intersection can be computed as a Kronecker product of the corresponding
%adjacency matrices for an RSM and a graph. Since we are only determining the
%reachability of vertices, it is enough to represent intersection result as
%a Boolean matrix. It simplifies the algorithm implementation and allows
%one to express it in terms of basic matrix operations.
% TODO: more accurate upper bound for the algorithm complexity

\subsubsection{Index creation for RPQ}
In the case of the RPQ, the main \textbf{while} loop takes only one iteration to actually append data.
Since the input query is provided in the form of the regular expression, one can construct the corresponding RSM which consists of the single \textit{component state machine}.
This CSM is built from the regular expression and is labeled as $S$, for example, and has no \textit{recursive calls}.
The~adjacency matrix of the machine is built over $\Sigma$ only.
Therefore, during the calculation of the Kronecker product, all relevant information is taken into account at the first iteration of the loop.