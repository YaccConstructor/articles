\subsection{Index Creation Algorithm}

The \textit{index creation} algorithm outputs the final adjacency matrix $\mathcal{M}_2$ for the input graph with all pairs of vertices which are reachable through some nonterminal in the input grammar $G$, as well as the index matrix $C_3$, which is to be used to extract paths in the \textit{path extraction} algorithm.

The algorithm is based on the generalization of the FSM intersection for an RSM,  and the edge-labeled directed input graph.
Since the RSM is composed as a set of FSMs, it could be easily presented as an adjacency matrix for some graph over labels set $\Sigma \cup S$.
As shown in the Def.~\ref{def:bool:product}, we can apply Kronecker product from Boolean matrices to \textit{intersect} the RSM and the input graph to some extent.
But the RSM contains the nonterminal symbols from $N$ with additional \textit{recursive calls} logic, which requires \textit{transitive closure} step to extract such symbols.

The core idea of the algorithm comes from Kronecker product and transitive closure.
The algorithm boils down to the iterative Kronecker product evaluation for the RSM adjacency matrix $\mathcal{M}_1$ and the input graph adjacency matrix $\mathcal{M}_2$, followed by transitive closure, extraction of nonterminals and updating the graph adjacency matrix $\mathcal{M}_2$.
Listing~\ref{tensor:cfpq:cubic} shows main steps of the algorithm.
\begin{algorithm}[h]
\floatname{algorithm}{Listing}
\begin{algorithmic}[1]
\footnotesize
\caption{Kronecker product based CFPQ using dynamic transitive closure}
\label{tensor:cfpq:cubic}
\Function{contextFreePathQuerying}{G, $\mathcal{G}$}
    % Input data preparation
    \State{$R \gets$ Recursive automata for $G$}
    \State{$M_1 \gets$ Adjacency matrix for $R$}
    \State{$M_2 \gets$ Adjacency matrix for $\mathcal{G}$}
    \State{$A_2 \gets$ Adjacency matrix for $\mathcal{G}$}
    %\State{$M_3 \gets$ The empty matrix}
    \State{$C_3 \gets$ The empty matrix}
    % Eps-transition handling for graph
    \For{$s \in 0..dim(M_1)-1$}
        \For{$i \in 0..dim(M_2)-1$}
            \State{$M_2[i,i] \gets M_2[i,i] \cup \textit{getNonterminals}(R,s,s)$}
        \EndFor
    \EndFor
    \While{Matrix $M_2$ is changing}
       \State{$M_3' \gets  M_1 \otimes A_2$}
        \State{$A_2 \gets$ The empty matrix of size $n \times n$}
        %\Comment{Evaluate Kroncker product}
       \For{$M_3'[i,j] \mid M_3'[i,j] = 1$}
            \State{$C_3[i,j] \gets 1$}
             \State{$C_3' \gets  \bigcup_{(i,j)} \textit{add}(C_3, i, j)$}
             \Comment{Updating the transitive closure}
             \State{$C_3 \gets C_3 + C_3'$}
        \EndFor
        \State{$n \gets$ dim($M_3)$}
        %\Comment{Matrix $M_3$ size = $n \times n$}
        % Add non-terminals (possibly new)
        \For{$(i,j)\ |\ C_3'[i,j] \neq 0$}
                \State{$s, f \gets \textit{getStates}(C_3',i,j)$}
                \If{$\textit{getNonterminals}(R,s,f) \neq \emptyset$}
                    \State{$x, y \gets \textit{getCoordinates}(C_3',i,j)$}
                    \State{$M_2[x,y] \gets M_2[x,y] \cup \textit{getNonterminals}(R,s,f)$}
                     \State{$A_2[x,y] \gets A_2[x,y] \cup \textit{getNonterminals}(R,s,f)$}
                \EndIf
        \EndFor
    \EndWhile
\State \Return $\mathcal{M}_2, C_3$
\EndFunction
\Function{getStates}{$C, i, j$}
    \State{$r \gets dim(\mathcal{M}_1)$}
    \Comment{$\mathcal{M}_1$ is adjacency matrices for $R$}
    \State \Return{$\left\lfloor{i / r}\right\rfloor, \left\lfloor{j / r}\right\rfloor$}
\EndFunction
\Function{getCoordinates}{$C, i, j$}
    \State{$n \gets dim(\mathcal{M}_2)$}
    \Comment{$\mathcal{M}_2$ is adjacency matrices for $\mathcal{G}$}
    \State \Return{$i \bmod n, j \bmod n$}
\EndFunction
\end{algorithmic}
\end{algorithm}
\subsubsection{Application of Dynamic Transitive Closure}
It is easy to see that the most time-consuming steps of the algorithm are the Kronecker product and transitive closure computations.
Note that the adjacency matrix $\mathcal{M}_2$ is always changed incrementally i. e. elements (edges) are added to $\mathcal{M}_2$ (and are never deleted from it) at each iteration of the algorithm.
So it is not necessary to recompute the whole product or transitive closure if an appropriate data structure is maintained.


To compute the Kronecker product, we employ the fact that it is left-distributive.
Let $\mathcal{A}_2$ be a matrix with newly added elements and $\mathcal{B}_2$ be a matrix with the all previously found elements, such that $\mathcal{M}_2 = \mathcal{A}_2 + \mathcal{B}_2$.
Then by the left-distributivity of the Kronecker product we have $\mathcal{M}_1 \otimes \mathcal{M}_2 = \mathcal{M}_1 \otimes (\mathcal{A}_2 + \mathcal{B}_2) = \mathcal{M}_1\otimes \mathcal{A}_2 + \mathcal{M}_1 \otimes \mathcal{B}_2$.
Note that $\mathcal{M}_1 \otimes \mathcal{B}_2$ is known and is already in the matrix $\mathcal{M}_3$ and its transitive closure also is already in the matrix $C_3$, because it has been calculated at the previous iterations, so it is left to update some elements of $\mathcal{M}_3$ by computing $\mathcal{M}_1\otimes \mathcal{A}_2$.


The fast computation of transitive closure can be obtained by using incremental dynamic transitive closure technique. Now we describe the function $add$ from Listing \ref{tensor:cfpq:cubic}. Let $C_3$ be a transitive closure matrix of the graph $G$ with $n$ vertices. We use an approach by Ibaraki and Katoh~\cite{IBARAKI198395} to maintain dynamic transitive closure. The key idea of their algorithm is to recalculate reachability information only for those vertices, which become reachable after insertion of the certain edge. We have modified it to achive a logarithmic speed-up.


For each newly inserted edge $(i, j)$ and every node $u \neq j$ of $G$ such that $C_3[u, i] = 1$ and $C_3[u, j]=0$, one needs to perform operation $C_3[u,v] = C_3[u, v] \wedge C_3[j, v]$ for every node $v$, where $1 \wedge 1 = 0 \wedge 0 = 1 \wedge 0 = 0$ and $0 \wedge 1 = 1$.
Notice that these operations are equivalent to the element-wise (Hadamard) product of two vectors of size $n$, where multiplication operation is denoted as $\wedge$. To check whether $C_3[u, i] = 1$ and $C_3[u, j]=0$ one needs to multiply two vectors: the first vector represents reachability of the given vertex $i$ from other vertices $\{u_1, u_2, ..., u_n\}$ of the graph and the second vector represents the same for the given vertex $j$. The operation $C_3[u, v] \wedge C_3[j, v]$ also can be reduced to the computation of the Hadamard product of two vectors of size $n$ for the given $u_k$. The first vector contains the information whether vertices  $\{v_1, v_2, ..., v_n\}$ of the graph are reachable from the given vertex $u_k$ and the second vector represents the same for the given vertex $j$. The element-wise product of two vectors can be calculated naively in time $O(n)$. Thus, the time complexity of the transitive closure can be reduced by speeding up element-wise product of two vectors of size $n$.


To achieve logarithmic speed-up, we use the Four Russians' trick.
First we split each vector into $n/\log n$ parts of size $\log n$.
Then we create a table $S$ such that $S(a, b)$ = $a \wedge b$ where $a, b \ \in {\{0,1\}}^{\log n}$.
This takes time $O(n^2 \log n)$, since there are $2^{\log n} = n$ variants of Boolean vectors of size $\log n$ and hence $n^2$ possible pairs of vectors $(a, b)$ in total, and each component takes $O(\log n)$ time.
With table $S$, we can calculate product of two parts of size $\log n$ in constant time.
There are $n/\log n$ such parts, so the element-wise product of two vectors of size $n$ can be calculated in time $O(n/\log n)$ with $O(n^2 \log n)$ preprocessing.

\begin{theorem}
    Let $\mathcal{G} = (V,E,L)$ be a graph and $G = \langle\Sigma, N, S, P\rangle$ be a grammar.
    Let $\mathcal{M}_{2}$ be a resulting adjacency matrix after the execution of the algorithm in Listing~\ref{tensor:cfpq:cubic}. Then for any valid indices $i, j$ and for each nonterminal $A \in N$ the following statement holds: the cell $M_{2,(k)}^A[i,j]$ contains $\{1\}$, iff there is a path $i\pi j$ in the graph $\mathcal{G}$ such that $ A \xrightarrow{*} l(\pi)$.
\end{theorem}{}
\begin{proof}
    The main idea of the proof is to use induction on the height of the derivation tree obtained on each iteration.
\end{proof}{}

\begin{theorem}{}
\label{theorem: subcubic}
    Let $\mathcal{G} = (V,E,L)$ be a graph and $G = \langle\Sigma, N, S, P\rangle$ be a grammar.
    The algorithm from Listing~\ref{tensor:cfpq:cubic} calculates resulting matrices $\mathcal{M}_2$ and $C_3$ in $O(n^3/\log n)$ time where $n = |V|$. Moreover, the maintaining of the dynamic transitive closure dominates the cost of the algorithm.
\end{theorem}

\begin{proof}
 Let $|\mathcal{A}|$ be a number of non-zero elements in a matrix $\mathcal{A}$. Consider the total time which is needed for computing the Kronecker products. The elements of the matrices $\mathcal{A}_2^{(i)}$ are pairwise distinct on every $i$-th iteration of the algorithm therefore the total number of operations is
 $$\sum\limits_i{T(\mathcal{M}_1 \otimes \mathcal{A}_2^{(i)})} = |\mathcal{M}_1| \otimes \sum\limits_i {|\mathcal{A}_2^{(i)}|} = |\mathcal{M}_1|O(n^2).$$


Now we derive the time complexity of maintainig the dynamic transitive closure.
Since $C_3$ has size of $O(n^2)$, no more than $O(n^2)$ edges will be added during all iterations of the algorithm.
Checking condition whether $C_3[u, i] = 1$ and $C_3[u, j]=0$ for every node $u \in V$ for each newly inserted edge $(i, j)$ requires one multiplication of vectors per insertion, thus total time is $O(n^3/\log n)$.
Note that after checking the condition, at least one element $C[u', j]$ changes value from 0 to 1 and then never becomes 0 for some $u'$ and $j$.
Therefore the operation $C_3[u',v] = C_3[u', v] \wedge C_3[j, v]$ for all $v \in V$ is executed at most once for every pair of vertices $(u',j)$ during the entire computation implying that the total time is equal to $O(n^2n/\log n)=O(n^3/\log n)$ (using multiplication of vectors).


The matrix $C_3'$ contains only new elements, therefore $C_3$ can be updated directly using only $|C_3'|$ operations and hence $O(n^2)$ operations in total.
The same holds for cycle in line 18 of the algorithm from Listing~\ref{tensor:cfpq:cubic}, because operations are performed only for non-zero elements of the matrix $|C_3'|$.
Finally, the time complexity of the algorithm is $O(n^2) + O(n^2 \log n) + O(n^3/\log n) + O(n^2) + O(n^2) = O(n^3/\log n)$.
\end{proof}{}

The complexity analysis of the Algorithm~\ref{tensor:cfpq:cubic} shows that the maintaining of the incremental transitive closure dominates the cost of the algorithm. Thus, CFPQ can be solved in truly subcubic $O(n^{3-\varepsilon})$ time if there is an incremental dynamic algorithm for the transitive closure for a graph with $n$ vertices with preprocessing time $O(n^{3-\varepsilon})$ and total update time $O(n^{3-\varepsilon})$. Unfortunately, such an algorithm is unlikely to exist: it was proven that there is no incremental dynamic transitive closure algorithm for a graph with $n$ vertices and at most $m$ edges with preprocessing time $poly(m)$, total update time $mn^{1-\varepsilon}$, and query time $m^{\delta-\varepsilon}$ for any $\delta \in (0, 1/2]$ per query that has an error probability of at most 1/3 assuming the widely believed Online Boolean Matrix-Vector Multiplication (OMv) Conjecture~\cite{10.1145/2746539.2746609}. OMv Conjecture introduced by Henzinger et al. ~\cite{10.1145/2746539.2746609} states that for any constant $ \varepsilon>0$, there is no $O(n^{3-\varepsilon})$-time algorithm that solves OMv with an error probability of at most 1/3. 

% In this section, we introduce the algorithm for context-free path querying.
%The algorithm determines the existence of a path, which forms a sentence of the language defined by
%the input RSM $R$, between each pair of vertices in the directed edge-labeled graph $\mathcal{G}$.
%The algorithm is based on the generalization of the FSM intersection for an RSM,
%and an input graph. Since a graph can be interpreted as a FSM, in which
%transitions correspond to the labeled edges between vertices of the graph,
%and an RSM is composed of a set of FSMs, the intersection of such machines
%can be computed using the classical algorithm for FSM intersection, presented
%in~\cite{automata:theory:10.5555/1177300}.
%Such a way of generalization leads to zero-overhead algorithm for RPQ, contrary to other algorithms which require regular expression to context-free grammar transformation.

%The intersection can be computed as a Kronecker product of the corresponding
%adjacency matrices for an RSM and a graph. Since we are only determining the
%reachability of vertices, it is enough to represent intersection result as
%a Boolean matrix. It simplifies the algorithm implementation and allows
%one to express it in terms of basic matrix operations.
% TODO: more accurate upper bound for the algorithm complexity

\subsubsection{Index creation for RPQ}
In case of the RPQ, the main \textbf{while} loop takes only one iteration to actually append data.
Since the input query is provided in the form of the regular expression, one can construct the corresponding RSM, which consists of the single \textit{component state machine}.
This CSM is built from the regular expression and is labeled as $S$, for example, which has no \textit{recursive calls}.
The adjacency matrix of the machine is build over $\Sigma$ only.
Therefore, calculating the Kronecker product, all relevant information is taken into account at the first iteration of the loop.