\documentclass[12pt]{article}  % standard LaTeX, 12 point type
\usepackage{amsfonts,latexsym}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[utf8x]{inputenc} % Кодировка
\usepackage[english]{babel} % Многоязычность
\usepackage{multirow}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}{Определение}[section]
\newtheorem{example}{Example}[section]

% unnumbered environments:

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{notation}{Notation}
\newtheorem*{note}{Note}

\setlength{\parskip}{5pt plus 2pt minus 1pt}
%\setlength{\parindent}{0pt}

\usepackage{color}
\usepackage{listings}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{ucs}
\usepackage{hyperref}
\usepackage{textcomp}

\newcommand{\tab}[1][0.3cm]{\ensuremath{\hspace*{#1}}}

\title{Secondary Structure Prediction by Combination of Formal Grammars and Neural Networks\footnote{The research was supported by the Russian Science Foundation grant 18-11-00100 and a grant from JetBrains Research.}}

\author{Semyon Grigorev, Dmitry Kutlenkov, Polina Lunina
\\ 
       \small{Saint Petersburg State University}\\
       \small{7/9 Universitetskaya nab., St. Petersburg, 199034, Russia}\\
       \small{JetBrains Research}\\
       \small{Primorskiy prospekt 68-70, Building 1, St. Petersburg, 197374, Russia} \\
       \small{semyon.grigorev@jetbrains.com, kutlenkov.dmitri@gmail.com, lunina\_polina@mail.ru}
       }
\date{}

\begin{document}
\maketitle
Secondary structure is known to have a crucial impact on RNA molecules functioning, therefore, development of algorithms for secondary structure modeling and prediction is a fundamental task in computational genomics. Comparative methods of secondary structure prediction analyse several homologous sequences employing evolutionary approaches, while single sequence methods process one sequence at a time, and the most popular approach here is to find the minimum free energy structure~\cite{hofacker1994fast,hamada2009prediction}. Also, secondary structure can be theoretically described by means of formal grammars~\cite{dowell2004evaluation,knudsen1999rna}.


An approach for sequences secondary structure analysis by combination of formal grammars and neural networks was proposed in ~\cite{grigorev2019composition,improved}. In this work, we investigate the possibilities of this approach for RNA secondary structure prediction. Secondary structure can be described as a composition of stems having different heights and loop sizes~\cite{MQbioinformatics19}. We use context-free grammar $G_0$ from~\cite{grigorev2019composition,improved} to encode the most common kinds of stems and parsing algorithm~\cite{Azimov:2018:CPQ:3210259.3210264} to find the subsequences of a given sequence that should fold to such stems. Note that this grammar describes only the classical base pairs and cannot express pseudoknots. The result of a matrix-based parsing algorithm for string $w$ is a boolean matrix $M$, where $M[i,j] = 1$, iff the substring $w[i,j-1]$ is derivable from grammar start nonterminal, i.e. folds to a stem. Suchwise we get a representation of all the theoretically possible stems in terms of $G_0$, but the real secondary structure cannot contain all of them at once and, besides, there are more complex elements (such as pseudoknots) that are not expressible in given grammar. Therefore, parsing matrices require further processing and we propose to use a neural network to handle them in order to generate an actual secondary structure. 

For experimental research we took sequences from RnaCentral~\cite{rnacentral} database and as a reference data for a network we used contact matrices generated by CentroidFold tool~\cite{hamada2009prediction}, where $[i,j]$ element of matrix is equal to $1$ iff there is a connection between nucleotides $i$ and $j$ in secondary structure. We transformed parsing matrices as well as contact maps to black-and-white images, where white pixel in position $[i,j]$ corresponds to $1$ in matrix and black --- to $0$. These images were used for training the generative neural network which should take parsing-provided image as an input and transform it to the maximal approximation of the considered contact map. We applied deep residual networks with the local alignment algorithm at the end of sequence of layers.

We trained models on several datasets with fixed sequences length interval. Trained models were estimated by precision, recall and f1 score metrics calculated for numbers of correctly and incorrectly guessed contacts for each image. Results for different sequences lengths for models with and without alignment are presented in the table~\ref{table1}.

\begin{table}[h]
\centering
\begin{tabular}{|P{3cm}|P{2cm}|P{2cm}|P{2cm}|P{2cm}|}
\hline 
Sequence length & Alignment & Precision & Recall & F1 score \\ \hline \hline
\multirow{2}{*}{90} & $\times$ & 67\% & 75\% & 68\% \\ \cline{2-5} 
 & \checkmark & 80\% & 66\% & 70\% \\ \hline \hline
\multirow{2}{*}{88-90} & $\times$ & 66\% & 78\% & 69\% \\ \cline{2-5} 
 & \checkmark & 81\% & 62\% & 68\% \\ \hline \hline
50-90 & $\times$ & 60\% & 72\% & 63\% \\ \hline
\end{tabular}
\caption{Test results for all trained models}
\label{table1}
\end{table}


\bibliography{main} 
\bibliographystyle{ieeetr}


\end{document}