@misc{yang2019graphblast,
    title={GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on the GPU},
    author={Carl Yang and Aydin Buluc and John D. Owens},
    year={2019},
    eprint={1908.01407},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}

@article{Coimbra2021,
  doi = {10.1186/s40537-021-00443-9},
  url = {https://doi.org/10.1186/s40537-021-00443-9},
  year = {2021},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {8},
  number = {1},
  author = {Miguel E. Coimbra and Alexandre P. Francisco and Lu{\'{\i}}s Veiga},
  title = {An analysis of the graph processing landscape},
  journal = {Journal of Big Data}
}

@INPROCEEDINGS{7761646,  author={Kepner, Jeremy and Aaltonen, Peter and Bader, David and Buluç, Aydin and Franchetti, Franz and Gilbert, John and Hutchison, Dylan and Kumar, Manoj and Lumsdaine, Andrew and Meyerhenke, Henning and McMillan, Scott and Yang, Carl and Owens, John D. and Zalewski, Marcin and Mattson, Timothy and Moreira, Jose},  booktitle={2016 IEEE High Performance Extreme Computing Conference (HPEC)},   title={Mathematical foundations of the GraphBLAS},   year={2016},  volume={},  number={},  pages={1-9},  doi={10.1109/HPEC.2016.7761646}}

% Boost compute library
@inproceedings{10.1145/2909437.2909454:boost:compute,
    author = {Szuppe, Jakub},
    title = {Boost.Compute: A Parallel Computing Library for C++ Based on OpenCL},
    year = {2016},
    isbn = {9781450343381},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2909437.2909454},
    doi = {10.1145/2909437.2909454},
    abstract = {Boost.Compute is a powerful C++ header-only template library for parallel computing based on OpenCL. It has a layered architecture and acts both as a thin C++ wrapper over the OpenCL API and as a feature-rich interface to high-level constructs that resemble the functionality of the STL and is just as easy to use.Through its template-based design, Boost.Compute seeks to further provide flexible OpenCL support in C++ projects via meta-programming. Meta-programming is not available in standard C-based OpenCL, and OpenCL C++ kernel language is currently only provisional. SYCL is intended as a prospective C++ abstraction layer over OpenCL, but it is still under development and requires a special compiler. Bolt and Thrust are C++-based parallel-computing libraries, but they are tied to specific hardware vendors. Boost.Compute on the other hand is a vendor-neutral solution for GPUs and multi-core CPUs that is available now, and it works with any standard C++ compiler and on all OpenCL platforms.Boost.Compute has been accepted for integration with the official Boost C++ libraries. With this step, and considering the large number of Boost users, usage of Boost.Compute and visibility of OpenCL among C++ developers is likely to increase. This technical presentation is therefore intended as a comprehensive overview of Boost.Compute for current and prospective users of the library and covers the library's overall architecture, its low-level and high-level functionality and advanced topics such as custom functions, closures and lambda expressions. The presentation also describes how a custom template-based OpenCL library can be designed on top of Boost.Compute. Examples are included throughout the presentation to aid in a better understanding. Among others, I will demonstrate how advanced features of the library can lead to a simple and efficient C++-only solution for BLAS calculations. The architectural presentation of the library will be followed by a presentation of current performance results of the library and a comparison with competing solutions. I will conclude the presentation with insights that I gained during Google Summer of Code '15 and my overall experience in contributing to Boost.Compute, which I hope to be of interest to the wider developer community.},
    booktitle = {Proceedings of the 4th International Workshop on OpenCL},
    articleno = {15},
    numpages = {39},
    keywords = {Parallel computing, Boost Compute, Boost C++ Libraries, C++, OpenCL},
    location = {Vienna, Austria},
    series = {IWOCL '16}
}

% Fast SpVSpM
@INPROCEEDINGS{7284398:spvspm,
  author={Yang, Carl and Wang, Yangzihao and Owens, John D.},
  booktitle={2015 IEEE International Parallel and Distributed Processing Symposium Workshop}, 
  title={Fast Sparse Matrix and Sparse Vector Multiplication Algorithm on the GPU}, 
  year={2015},
  volume={},
  number={},
  pages={841-847},
  doi={10.1109/IPDPSW.2015.77}}

% Push-pull efficient
@misc{https://doi.org/10.48550/arxiv.1804.03327:pushpull,
  doi = {10.48550/ARXIV.1804.03327},
  url = {https://arxiv.org/abs/1804.03327},
  author = {Yang, Carl and Buluc, Aydin and Owens, John D.},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Implementing Push-Pull Efficiently in GraphBLAS},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


% GPU Merge path
@inproceedings{inproceedings:gpu_merge_path,
  author = {Green, Oded and McColl, Robert and Bader, David A.},
  title = {GPU Merge Path: A GPU Merging Algorithm},
  year = {2012},
  isbn = {9781450313162},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2304576.2304621},
  doi = {10.1145/2304576.2304621},
  abstract = {Graphics Processing Units (GPUs) have become ideal candidates for the development of fine-grain parallel algorithms as the number of processing elements per GPU increases. In addition to the increase in cores per system, new memory hierarchies and increased bandwidth have been developed that allow for significant performance improvement when computation is performed using certain types of memory access patterns.Merging two sorted arrays is a useful primitive and is a basic building block for numerous applications such as joining database queries, merging adjacency lists in graphs, and set intersection. An efficient parallel merging algorithm partitions the sorted input arrays into sets of non-overlapping sub-arrays that can be independently merged on multiple cores. For optimal performance, the partitioning should be done in parallel and should divide the input arrays such that each core receives an equal size of data to merge.In this paper, we present an algorithm that partitions the workload equally amongst the GPU Streaming Multi-processors (SM). Following this, we show how each SM performs a parallel merge and how to divide the work so that all the GPU's Streaming Processors (SP) are utilized. All stages in this algorithm are parallel. The new algorithm demonstrates good utilization of the GPU memory hierarchy. This approach demonstrates an average of 20X and 50X speedup over a sequential merge on the x86 platform for integer and floating point, respectively. Our implementation is 10X faster than the fast parallel merge supplied in the CUDA Thrust library.},
  booktitle = {Proceedings of the 26th ACM International Conference on Supercomputing},
  pages = {331–340},
  numpages = {10},
  keywords = {graphics processors, measurement of multiple-processor systems, parallel algorithms, parallel systems},
  location = {San Servolo Island, Venice, Italy},
  series = {ICS '12}
}

@article{10.1145/3322125,
author = {Davis, Timothy A.},
title = {Algorithm 1000: SuiteSparse:GraphBLAS: Graph Algorithms in the Language of Sparse Linear Algebra},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3322125},
doi = {10.1145/3322125},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {44},
numpages = {25},
keywords = {sparse matrices, GraphBLAS, Graph algorithms}
}

@INPROCEEDINGS{7529957,
  author={Zhang, Peter and Zalewski, Marcin and Lumsdaine, Andrew and Misurda, Samantha and McMillan, Scott},
  booktitle={2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={GBTL-CUDA: Graph Algorithms and Primitives for GPUs}, 
  year={2016},
  volume={},
  number={},
  pages={912-920},
  doi={10.1109/IPDPSW.2016.185}}

@inproceedings{10.1145/2600212.2600227,
author = {Khorasani, Farzad and Vora, Keval and Gupta, Rajiv and Bhuyan, Laxmi N.},
title = {CuSha: Vertex-Centric Graph Processing on GPUs},
year = {2014},
isbn = {9781450327497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600212.2600227},
doi = {10.1145/2600212.2600227},
abstract = {Vertex-centric graph processing is employed by many popular algorithms (e.g., PageRank) due to its simplicity and efficient use of asynchronous parallelism. The high compute power provided by SIMT architecture presents an opportunity for accelerating these algorithms using GPUs. Prior works of graph processing on a GPU employ Compressed Sparse Row (CSR) form for its space-efficiency; however, CSR suffers from irregular memory accesses and GPU underutilization that limit its performance. In this paper, we present CuSha, a CUDA-based graph processing framework that overcomes the above obstacle via use of two novel graph representations: G-Shards and Concatenated Windows (CW). G-Shards uses a concept recently introduced for non-GPU systems that organizes a graph into autonomous sets of ordered edges called shards. CuSha's mapping of GPU hardware resources on to shards allows fully coalesced memory accesses. CW is a novel representation that enhances the use of shards to achieve higher GPU utilization for processing sparse graphs. Finally, CuSha fully utilizes the GPU power by processing multiple shards in parallel on GPU's streaming multiprocessors. For ease of programming, CuSha allows the user to define the vertex-centric computation and plug it into its framework for parallel processing of large graphs. Our experiments show that CuSha provides significant speedups over the state-of-the-art CSR-based virtual warp-centric method for processing graphs on GPUs.},
booktitle = {Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {239–252},
numpages = {14},
keywords = {graph representation, coalesced memory accesses, concatenated windows, g-shards, gpu},
location = {Vancouver, BC, Canada},
series = {HPDC '14}
}

@INPROCEEDINGS{7967137,  author={Pan, Yuechao and Wang, Yangzihao and Wu, Yuduo and Yang, Carl and Owens, John D.},  booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},   title={Multi-GPU Graph Analytics},   year={2017},  volume={},  number={},  pages={479-490},  doi={10.1109/IPDPS.2017.117}}

% SuiteSparse Sparse Matrix Collection
@article{dataset:10.1145/2049662.2049663,
  author = {Davis, Timothy A. and Hu, Yifan},
  title = {The University of Florida Sparse Matrix Collection},
  year = {2011},
  issue_date = {November 2011},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {38},
  number = {1},
  issn = {0098-3500},
  url = {https://doi.org/10.1145/2049662.2049663},
  doi = {10.1145/2049662.2049663},
  abstract = {We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing and managing the Collection, from MATLAB™, Mathematica™, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task.},
  journal = {ACM Trans. Math. Softw.},
  month = {dec},
  articleno = {1},
  numpages = {25},
  keywords = {performance evaluation, Graph drawing, sparse matrices, multilevel algorithms}
}

% La-Graph
@misc{szarnyas2021lagraph,
  title={LAGraph: Linear Algebra, Network Analysis Libraries, and the Study of Graph Algorithms}, 
  author={Gábor Szárnyas and David A. Bader and Timothy A. Davis and James Kitchen and Timothy G. Mattson and Scott McMillan and Erik Welch},
  year={2021},
  eprint={2104.01661},
  archivePrefix={arXiv},
  primaryClass={cs.MS}
}

% Graphalytics benchmarks
@misc{Graphalytics:iosup2021ldbc,
  title={The LDBC Graphalytics Benchmark}, 
  author={Alexandru Iosup and Ahmed Musaafir and Alexandru Uta and Arnau Prat Pérez and Gábor Szárnyas and Hassan Chafi and Ilie Gabriel Tănase and Lifeng Nai and Michael Anderson and Mihai Capotă and Narayanan Sundaram and Peter Boncz and Siegfried Depner and Stijn Heldens and Thomas Manhardt and Tim Hegeman and Wing Lung Ngai and Yinglong Xia},
  year={2021},
  eprint={2011.15028},
  archivePrefix={arXiv},
  primaryClass={cs.DC}
}

% What did graphblas wrong
@article{talk:graphblas_did_wrong,
    author = {John R. Gilbert},
    title = {What did the GraphBLAS get wrong?},
    year = {2022},
    url = {https://sites.cs.ucsb.edu/~gilbert/talks/talks.htm},
    journal = {HPEC GraphBLAS BoF}
}

% Graph frameworks survey 
@article{article:batarfi_survey_graphs,
    author = {Batarfi, Omar and Shawi, Radwa El and Fayoumi, Ayman G. and Nouri, Reza and Beheshti, Seyed-Mehdi-Reza and Barnawi, Ahmed and Sakr, Sherif},
    title = {Large Scale Graph Processing Systems: Survey and an Experimental Evaluation},
    year = {2015},
    issue_date = {September 2015},
    publisher = {Kluwer Academic Publishers},
    address = {USA},
    volume = {18},
    number = {3},
    issn = {1386-7857},
    url = {https://doi.org/10.1007/s10586-015-0472-6},
    doi = {10.1007/s10586-015-0472-6},
    abstract = {Graph is a fundamental data structure that captures relationships between different data entities. In practice, graphs are widely used for modeling complicated data in different application domains such as social networks, protein networks, transportation networks, bibliographical networks, knowledge bases and many more. Currently, graphs with millions and billions of nodes and edges have become very common. In principle, graph analytics is an important big data discovery technique. Therefore, with the increasing abundance of large graphs, designing scalable systems for processing and analyzing large scale graphs has become one of the most timely problems facing the big data research community. In general, scalable processing of big graphs is a challenging task due to their size and the inherent irregular structure of graph computations. Thus, in recent years, we have witnessed an unprecedented interest in building big graph processing systems that attempted to tackle these challenges. In this article, we provide a comprehensive survey over the state-of-the-art of large scale graph processing platforms. In addition, we present an extensive experimental study of five popular systems in this domain, namely, GraphChi, Apache Giraph, GPS, GraphLab and GraphX. In particular, we report and analyze the performance characteristics of these systems using five common graph processing algorithms and seven large graph datasets. Finally, we identify a set of the current open research challenges and discuss some promising directions for future research in the domain of large scale graph processing.},
    journal = {Cluster Computing},
    month = {sep},
    pages = {1189–1213},
    numpages = {25},
    keywords = {Big graph, Experimental evaluation, Graph processing}
}

@article{article:shi_survey_graphs,
    author = {Shi, Xuanhua and Zheng, Zhigao and Zhou, Yongluan and Jin, Hai and He, Ligang and Liu, Bo and Hua, Qiang-Sheng},
    title = {Graph Processing on GPUs: A Survey},
    year = {2018},
    issue_date = {November 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {50},
    number = {6},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3128571},
    doi = {10.1145/3128571},
    abstract = {In the big data era, much real-world data can be naturally represented as graphs. Consequently, many application domains can be modeled as graph processing. Graph processing, especially the processing of the large-scale graphs with the number of vertices and edges in the order of billions or even hundreds of billions, has attracted much attention in both industry and academia. It still remains a great challenge to process such large-scale graphs. Researchers have been seeking for new possible solutions. Because of the massive degree of parallelism and the high memory access bandwidth in GPU, utilizing GPU to accelerate graph processing proves to be a promising solution. This article surveys the key issues of graph processing on GPUs, including data layout, memory access pattern, workload mapping, and specific GPU programming. In this article, we summarize the state-of-the-art research on GPU-based graph processing, analyze the existing challenges in detail, and explore the research opportunities for the future.},
    journal = {ACM Comput. Surv.},
    month = {jan},
    articleno = {81},
    numpages = {35},
    keywords = {Graph processing, GAS model, GPU, parallelism, BSP model, graph datasets}
}

% Vertex centric
@inproceedings{article:pregel,
    author = {Malewicz, Grzegorz and Austern, Matthew H. and Bik, Aart J.C and Dehnert, James C. and Horn, Ilan and Leiser, Naty and Czajkowski, Grzegorz},
    title = {Pregel: A System for Large-Scale Graph Processing},
    year = {2010},
    isbn = {9781450300322},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1807167.1807184},
    doi = {10.1145/1807167.1807184},
    abstract = {Many practical computing problems concern large graphs. Standard examples include the Web graph and various social networks. The scale of these graphs - in some cases billions of vertices, trillions of edges - poses challenges to their efficient processing. In this paper we present a computational model suitable for this task. Programs are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology. This vertex-centric approach is flexible enough to express a broad set of algorithms. The model has been designed for efficient, scalable and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronicity makes reasoning about programs easier. Distribution-related details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.},
    booktitle = {Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data},
    pages = {135–146},
    numpages = {12},
    keywords = {distributed computing, graph algorigthms},
    location = {Indianapolis, Indiana, USA},
    series = {SIGMOD '10}
}

% X-stream edge centric
@inproceedings{article:xstream,
    author = {Roy, Amitabha and Mihailovic, Ivo and Zwaenepoel, Willy},
    title = {X-Stream: Edge-Centric Graph Processing Using Streaming Partitions},
    year = {2013},
    isbn = {9781450323888},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2517349.2522740},
    doi = {10.1145/2517349.2522740},
    abstract = {X-Stream is a system for processing both in-memory and out-of-core graphs on a single shared-memory machine. While retaining the scatter-gather programming model with state stored in the vertices, X-Stream is novel in (i) using an edge-centric rather than a vertex-centric implementation of this model, and (ii) streaming completely unordered edge lists rather than performing random access. This design is motivated by the fact that sequential bandwidth for all storage media (main memory, SSD, and magnetic disk) is substantially larger than random access bandwidth.We demonstrate that a large number of graph algorithms can be expressed using the edge-centric scatter-gather model. The resulting implementations scale well in terms of number of cores, in terms of number of I/O devices, and across different storage media. X-Stream competes favorably with existing systems for graph processing. Besides sequential access, we identify as one of the main contributors to better performance the fact that X-Stream does not need to sort edge lists during preprocessing.},
    booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
    pages = {472–488},
    numpages = {17},
    location = {Farminton, Pennsylvania},
    series = {SOSP '13}
}

% comb blas
@article{article:combblas,
    author = {Bulu\c{c}, Ayd\i{}n and Gilbert, John R},
    title = {The Combinatorial BLAS: Design, Implementation, and Applications},
    year = {2011},
    issue_date = {November  2011},
    publisher = {Sage Publications, Inc.},
    address = {USA},
    volume = {25},
    number = {4},
    issn = {1094-3420},
    url = {https://doi.org/10.1177/1094342011403516},
    doi = {10.1177/1094342011403516},
    abstract = {This paper presents a scalable high-performance software library to be used for graph analysis and data mining. Large combinatorial graphs appear in many applications of high-performance computing, including computational biology, informatics, analytics, web search, dynamical systems, and sparse matrix methods. Graph computations are difficult to parallelize using traditional approaches due to their irregular nature and low operational intensity. Many graph computations, however, contain sufficient coarse-grained parallelism for thousands of processors, which can be uncovered by using the right primitives. We describe the parallel Combinatorial BLAS, which consists of a small but powerful set of linear algebra primitives specifically targeting graph and data mining applications. We provide an extensible library interface and some guiding principles for future development. The library is evaluated using two important graph algorithms, in terms of both performance and ease-of-use. The scalability and raw performance of the example applications, using the Combinatorial BLAS, are unprecedented on distributed memory clusters.},
    journal = {Int. J. High Perform. Comput. Appl.},
    month = {nov},
    pages = {496–509},
    numpages = {14},
    keywords = {Markov clustering, parallel graph library, graph analysis, combinatorial BLAS, sparse matrices, software framework, Betweenness centrality, mathematical software, combinatorial scientific computing}
}

% Huawei graphblas
@unpublished{article:hu_graphblas_impl,
     author = "Yzelman, A. N. and Di Nardo, D. and Nash, J. M. and Suijlen, W. J.",
     title = "A {C++} {GraphBLAS}: specification, implementation, parallelisation, and evaluation",
     year = "2020",
     note="Preprint",
     url={http://albert-jan.yzelman.net/PDFs/yzelman20.pdf}
}


@inproceedings{10.1145/2621934.2621936/MapGraph,
author = {Fu, Zhisong and Personick, Michael and Thompson, Bryan},
title = {MapGraph: A High Level API for Fast Development of High Performance Graph Analytics on GPUs},
year = {2014},
isbn = {9781450329828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2621934.2621936},
doi = {10.1145/2621934.2621936},
abstract = {High performance graph analytics are critical for a long list of application domains. In recent years, the rapid advancement of many-core processors, in particular graphical processing units (GPUs), has sparked a broad interest in developing high performance parallel graph programs on these architectures. However, the SIMT architecture used in GPUs places particular constraints on both the design and implementation of the algorithms and data structures, making the development of such programs difficult and time-consuming.We present MapGraph, a high performance parallel graph programming framework that delivers up to 3 billion Traversed Edges Per Second (TEPS) on a GPU. MapGraph provides a high-level abstraction that makes it easy to write graph programs and obtain good parallel speedups on GPUs. To deliver high performance, MapGraph dynamically chooses among different scheduling strategies depending on the size of the frontier and the size of the adjacency lists for the vertices in the frontier. In addition, a Structure Of Arrays (SOA) pattern is used to ensure coalesced memory access. Our experiments show that, for many graph analytics algorithms, an implementation, with our abstraction, is up to two orders of magnitude faster than a parallel CPU implementation and is comparable to state-of-the-art, manually optimized GPU implementations. In addition, with our abstraction, new graph analytics can be developed with relatively little effort.},
booktitle = {Proceedings of Workshop on GRAph Data Management Experiences and Systems},
pages = {1–6},
numpages = {6},
keywords = {GPU, Graph analytics, high-level API},
location = {Snowbird, UT, USA},
series = {GRADES'14}
}

@ARTICLE{6497047/Medusa,
  author={Zhong, Jianlong and He, Bingsheng},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Medusa: Simplified Graph Processing on GPUs}, 
  year={2014},
  volume={25},
  number={6},
  pages={1543-1552},
  doi={10.1109/TPDS.2013.111}}

@INPROCEEDINGS{8025284/spgemm/nagasaka,
  author={Nagasaka, Yusuke and Nukada, Akira and Matsuoka, Satoshi},
  booktitle={2017 46th International Conference on Parallel Processing (ICPP)}, 
  title={High-Performance and Memory-Saving Sparse General Matrix-Matrix Multiplication for NVIDIA Pascal GPU}, 
  year={2017},
  volume={},
  number={},
  pages={101-110},
  doi={10.1109/ICPP.2017.19}}
