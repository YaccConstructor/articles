\section{Introduction}
Development of effective computational methods for genomic sequences analysis is an open problem in bioinformatics.
While the existing algorithms for sequences classification and subsequences detection adopt different concepts and approaches, most of them share one idea: the secondary structure of genomic sequences contains important information about the biological functions of organisms.
There are different ways to handle secondary structure, for example, probabilistic grammars~\cite{dowell2004evaluation,knudsen1999rna} and covariance models~\cite{10.1093/nar/22.11.2079,EddyDurbin}.

Real-world biological data commonly contains different mutations, noise, and random variations.
This issue requires some sort of probability estimation while modeling the secondary structure.
Probabilistic grammars and covariance models provide such functionality, are expressive and handle long-distance connections.
They are successfully used in practical tools, such as Infernal~\cite{Infernal}, but building and training accurate grammar or model for predicting the whole secondary structure involves theoretical and practical difficulties.
On the other hand, artificial neural networks are a common way to process noisy data and find complex structural patterns.
Moreover, the efficiency of neural networks for genetic data processing has already been shown in some works~\cite{Humidor,ANN,Lu2019,Singh2019,10.5555/166459.166465}.

An approach for biological sequences processing which employs the combination of ordinary formal grammars and artificial neural networks was proposed in~\cite{grigorevcomposition}.  
The key idea is to use an ordinary (not probabilistic)  context-free grammar to describe only basic secondary structure features and leave the entire sequence analysis along with probabilistic estimation to the neural network which takes parsing-provided data as an input and solves some given task.
In this paper, we improve this solution. 
We provide some ideas that are aimed to optimize its quality and performance and solve the problems that we faced during the experimental research.

Namely, we do the following contribution in this work.
\begin{itemize}
\item We propose to represent parsing matrices as bitmap images to preserve data locality and to process such images using convolution layers. 
A dense neural network is used in the original solution, so parsing matrix vectorization is required to fit data format. 
As a result, information about contacts arrangement is implicit which makes training harder.
\item  We propose a way to train a network which handles sequences instead of parsing result.
A network which handles parsing result is proposed in the original work. But parsing is a very time-consuming step. 
We use parsing only for network training, and as a result, improve the performance of the final solution.
\item We provide an evaluation of the both proposed improvements on tRNA classification tasks. 
We show that, first of all, convolution layers utilization improves the quality of our solution in terms of networks accuracy, precision and recall, and decreases the total training time.
Also, we show that parsing elimination significantly improves the performance of the final solution without the degradation of its quality.
\end{itemize}