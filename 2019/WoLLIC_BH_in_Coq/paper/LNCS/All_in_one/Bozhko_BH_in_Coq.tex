% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

%\usepackage{minted}
\usepackage{verbments}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{listings}

\usepackage{url}


\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


\begin{document}
%
\title{Bar-Hillel Theorem Mechanization in Coq\thanks{The research was supported by the Russian Science Foundation, grant \textnumero 18-11-00100.}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Sergey Bozhko\inst{1} \and
Leyla Khatbullina\inst{2} \and
Semyon Grigorev\inst{3,4}\orcidID{0000-0002-7966-0698}}
%
\authorrunning{S. Bozko et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Max Planck Institute for Software Systems (MPI-SWS), Saarbrücken, Germany \\
\email{sbozhko@mpi-sws.com}
\and
St.Petersburg Electrotechnical University ``LETI'', St.Petersburg, Russia\\
\email{leila.xr@gmail.com}
\and
St.Petersburg State University, 7/9 Universitetskaya nab., St.Petersburg, Russia\\
\email{s.v.grigoriev@spbu.ru}
\and
JetBrains Research, Universitetskaya emb., 7-9-11/5A, St.Petersburg, Russia
\email{semen.grigorev@jetbrains.com}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Formal language theory has a deep connection with such areas as static code analysis, graph database querying, formal verification, and compressed data processing.
Many application problems can be formulated in terms of languages intersection.
The Bar-Hillel theorem states that context-free languages are closed under intersection with a regular set.
This theorem has a constructive proof and thus provides a formal justification of correctness of the algorithms for applications mentioned above.
Mechanization of the Bar-Hillel theorem, therefore, is both a fundamental result of formal language theory and a basis for the certified implementation of the algorithms for applications.
In this work, we present the mechanized proof of the Bar-Hillel theorem in Coq.

\keywords{Formal languages \and Coq \and Bar-Hillel theorem \and Closure \and Intersection \and Regular Language \and Context-free language.}
\end{abstract}
%
%
%
\section{Introduction}

Formal language theory has a deep connection with different areas such as static code analysis~\cite{Reps:1995:PID:199448.199462,vardoulakis2010cfa2,Yan:2011:DCA:2001420.2001440,rehof2001type,lu2013incremental,pratikakis2006existential,zhang2017context}, graph database querying~\cite{hellingsRelational,hellingsPathQuerying,zhang2016context,koschmieder2012regular}, formal verification~\cite{10.1007/11730637_17,10.1007/3-540-63141-0_10}, and others.
One of the most frequent uses is to formulate a problem in terms of languages intersection.
In verification, one language can serve as a model of a program and another language describe undesirable behaviors.
When the intersection of these two languages is not empty, one can conclude that the program is incorrect.
Usually, the only concern is the decidability of the languages intersection emptiness problem.
But in some cases, a constructive representation of the intersection may prove useful.
This is the case, for example, when the intersection of the languages models graph querying: a language produced by intersection is a query result and to be able to process it, one needs the appropriate representation of the intersection result.

Let us consider several applications starting with the user input validation.
The problem is to check if the input provided by the user is correct with respect to some validation template such as a regular expression for e-mail validation.
User input can be represented as a one word language.
The intersection of such a language with the language specifying the validation template is either empty or contains the only string: the user input.
If the intersection is empty, then the input should be rejected.

Checking that a program is syntactically correct is another example.
The AST for the program (or lack thereof) is just a constructive representation of the intersection of the one-word language (the program) and the programming language itself.

Graph database regular querying serves as an example of the intersection of two regular languages~\cite{ABITEBOUL1999428,koschmieder2012regular,alkhateeb:tel-00293206}.
Next and one of the most comprehensive cases with decidable emptiness problem is an intersection of a regular language with a context-free language.
This case is relevant for program analysis~\cite{Reps:1995:PID:199448.199462,vardoulakis2010cfa2,Yan:2011:DCA:2001420.2001440}, graph analysis~\cite{hellingsPathQuerying,zhang2016context,grigorev2016context}, context-free compressed data processing~\cite{MANETH201819}, and other areas.
The constructive intersection representation in these applications is helpful for further analysis.

The intersection of some classes of languages is not generally decidable.
For example, the intersection of the linear conjunctive and the regular languages, used in the static code analysis~\cite{zhang2017context}, is undecidable while multiple context-free languages (MCFL) is closed under intersection with regular languages and emptiness problem for MCFLs is decidable~\cite{SEKI1991191}.
Is it possible to express any useful properties in terms of regular and multiple context-free languages intersection?
This question is beyond the scope of this paper but provides a good reason for future research in this area.
Moreover, the history of pumping lemma for MCFG shows the necessity to mechanize formal language theory.
In this paper, we focus on the intersection of regular and context-free languages.

Some applications mentioned above require certifications.
For verification this requirement is evident.
For databases it is necessary to reason about security aspects and, thus, we should create certified solutions for query executing.
Certified parsing may be critical for secure data loading (for example in Web), as well as certified regular expressions for input validation.
As a result, there is a significant number of papers focusing on regular expressions mechanization and certification~\cite{10.1007/978-3-319-03545-1_7}, and a number on certified parsers~\cite{10.1007/978-3-642-00590-9_12,firsov2014certified,Gross2015ParsingPA}.
On the other hand, mechanization (formalization) is important by itself as theoretical results mechanization and verification, and there is a lot of work done on formal languages theory mechanization~\cite{firsov2015certified,ramos2016formalization,1885-16399}.
Also, it is desirable to have a base to reason about parsing algorithms and other problems of languages intersection.

Context-free languages are closed under intersection with regular languages.
It is stated as the Bar-Hillel theorem~\cite{bar1961formal} which provides a constructive proof and construction for the resulting language description.
We believe that the mechanization of the Bar-Hillel theorem is a good starting point for certified application development and since it is one of the fundamental theorems, it is an important part of formal language theory mechanization.
And this work aims to provide such mechanization in Coq.

Our current work is the first step: we provide mechanization of theoretical results on context-free and regular languages intersection.
We choose the result of Jana Hofmann on context-free languages mechanization~\cite{smolkaHofmann2016} as a base for our work.
The main contribution of this paper is the constructive proof of the Bar-Hillel theorem in Coq. All code is published on GitHub: \url{https://github.com/YaccConstructor/YC_in_Coq}.

\section{Bar-Hillel Theorem}
\label{sec:b-h-th}

In this section, we provide the Bar-Hillel theorem and sketch the proof which we use as the base of our work.
We also provide some additional lemmas which are used in the proof of the main theorem.

\begin{lemma} \label{l1}
	If $\boldsymbol{L}$ is a context-free language and $\boldsymbol{\varepsilon} \notin \boldsymbol{L}$ then there is a grammar in Chomsky Normal Form that generates $\boldsymbol{L}$.
\end{lemma}

\begin{lemma} \label{l2}
	If $ \boldsymbol{L} \neq \varnothing $ and $\boldsymbol{L}$ is regular then $\boldsymbol{L}$ is the union of regular language $\boldsymbol{A_1}, \ldots , \boldsymbol{A_n}$ where each $\boldsymbol{A_i}$ is accepted by a DFA with precisely one final state.
\end{lemma}

\begin{theorem}[Bar-Hillel]
	If $\boldsymbol{L_1}$ is a context-free language and $\boldsymbol{L_2}$ is a regular language, then $\boldsymbol{L_1} \cap \boldsymbol{L_2}$ is context-free.
\end{theorem}


Sketch of the proof.
\begin{enumerate}
	\item By Lemma~\ref{l1} we can assume that there is a context-free grammar $G_{\text{CNF}}$ in Chomsky normal form, such that $L(G_{CNF}) = L_1$
	\item By Lemma~\ref{l2} we can assume that there is a set of regular languages $\{A_{1} \ldots A_n \}$ where each $A_i$ is recognized by a DFA with precisely one final state and $L_2 = A_1 \cup \ldots \cup A_n$
	\item For each $A_i$ we can explicitly define a grammar of the $L( G_{\text{CNF}} ) \cap A_i$
	\item Finally, we join them together with the union operation
\end{enumerate}

As far as Bar-Hillel theorem operates with arbitrary context-free languages and the selected proof requires grammar in CNF, it is necessary to implement a certified algorithm for the conversion of an arbitrary CF grammar to CNF.
We wanted to reuse existing mechanized proof for the conversion.
We chose the one provided in Smolka's work and discussed it in the context of our work in section~\ref{sec:solka-generalized}.

\section{Bar-Hillel Theorem Mechanization in Coq}
\label{sec:main}

In this section, we describe in detail all the fundamental parts of the proof.
We also briefly describe the motivation to use the chosen definitions.
In addition, we discuss the advantages and disadvantages of using third-party proofs.

The overall goal of this section is to provide a step-by-step algorithm which constructs the context-free grammar of the intersection of two languages.
The final formulation of the theorem can be found in the last subsection.

\subsection{ Hofmann's Results Generalization}
\label{sec:solka-generalized}

A substantial part of this proof relies on the work of Jana Hofmann~\cite{smolkaHofmann2016}\footnote{Jana Hofmann, Verified Algorithms for Context-Free Grammars in Coq. Related sources in Coq: \url{https://www.ps.uni-saarland.de/~hofmann/bachelor/coq_src.zip}. Documentation: \url{https://www.ps.uni-saarland.de/~hofmann/bachelor/coq/toc.html}. Access date: 10.10.2018.} from which many definitions and theorems were taken. Namely, the definition of a grammar, the definitions of a derivation in grammar, some auxiliary lemmas about the decidability of properties of grammar and derivation. We also use the theorem that states that there always exists the transformation from a context-free grammar to a grammar in Chomsky Normal Form.

However, the proof of the existence of the transformation to CNF had one major flaw that we needed to fix: the representation of terminals and nonterminals.
In the definition of the grammar, a terminal is an element of the set of terminals---the alphabet of terminals.
It is sufficient to represent each terminal by a unique natural number---conceptually, the index of the terminal in the alphabet.

The same observation is correct for nonterminals.
Sometimes it is useful when the alphabet of nonterminals bears some structure.
For the purposes of our proof, nonterminals are better represented as triples.
We decided to make terminals and nonterminals to be polymorphic over the alphabet.
We are only concerned that the representation of symbols is a type with decidable relation of equality.
Namely, let \textit{Tt} and \textit{Vt} be such types, then we can define the types of terminals and nonterminals over \textit{Tt} and \textit{Vt} respectively.

Fortunately, the proof of Hofmann has a clear structure, and there was only one aspect of the proof where the use of natural numbers was essential. The grammar transformation which eliminates long rules creates new nonterminals. In the original proof, it was done by taking the maximum of the nonterminals included in the grammar.
It is not possible to use the same mechanism for an arbitrary type.

To tackle this problem, we introduced an additional assumption on the alphabet types for terminals and nonterminals. We require the existence of the bijection between natural numbers and the alphabet of terminals as well as nonterminals.

Another difficulty is that the original work defines grammar as a list of rules and does not specify the start nonterminal. Thus, in order to define the language described by a  grammar, one needs to specify the start terminal explicitly. It leads to the fact that the theorem about the equivalence of a CF grammar and the corresponding CNF grammar is not formulated in the most general way, namely, it guarantees equivalence only for non-empty words.

The predicate ``is grammar in CNF'' as defined in Hofmann~\cite{smolkaHofmann2016} does not treat the case when the empty word is in the language. That is, with respect to the definition in~\cite{smolkaHofmann2016}, a grammar cannot have epsilon rules at all.

The question of whether the empty word is derivable is decidable for both the CF grammar and the DFA. Therefore, there is no need to adjust the definition of the grammar (and subsequently all proofs). It is possible just to consider two cases (1) when the empty word is derivable in the grammar (and acceptable by DFA) and (2) when the empty word is not derivable. We use this feature of CNF definition to prove some of the lemmas presented in this paper.

\subsection{Basic Definitions}

In this section, we introduce the basic definitions used in the paper, such as alphabets, context-free grammar, and derivation.

We define a symbol as either a terminal or a nonterminal.
Next, we define a word and a phrase as lists of terminals and symbols respectively.
One can think that word is an element of the language defined by the grammar, and a phrase is an intermediate result of derivation.
Also, a right-hand side of any derivation rule is a phrase.

The notion of nonterminal does not make sense for DFA, but in order to construct the derivation in grammar, we need to use nonterminals in intermediate states. For phrases, we introduce a predicate that defines whenever a phrase consists of only terminals. If it is the case, the phrase can be safely converted to the word.

We inherit the definition of CFG from~\cite{smolkaHofmann2016}. The rule is defined as a pair of a nonterminal and a phrase, and a grammar is a list of rules.
Note, that this definition of a grammar does not include the start nonterminal, and thus does not specify the language by itself.

An important step towards the definition of a language specified by a grammar is the definition of derivability. Proposition $der(G, A, p)$ means that the phrase $p$ is derivable in the grammar $G$ starting from the nonterminal $A$.

Also, we use the proof of the fact that every grammar is convertible into CNF from~\cite{smolkaHofmann2016} because this fact is important for our proof.

We define the language as follows. We say that a phrase (not a word) $ w $ belongs to the language generated by a grammar $G$ from a nonterminal $A$, if $ w $ is derivable from nonterminal $ A $ in grammar $ G $ and $ w $ consists only of terminals.


\subsection{General Scheme of the Proof}

A general scheme of our proof is based on the constructive proof presented in~\cite{beigelproof}.
This proof does not use push-down automata explicitly and operates with grammars, so it is pretty simple to mechanize it.
Overall, we will adhere to the following plan.

\begin{enumerate}
    \item We consider the trivial case when DFA has no states.
    \item We state that every CF language can be converted to CNF.
    \item We show that every DFA can be presented as a union of DFAs with the single final state.
    \item We construct an intersection of grammar in CNF with DFA with one final state.
    \item We prove that the union of CF languages is CF language.
    \item We putting everything mentioned above together.
	Additionally, we handle the fact that the initial CF language may contain the $\varepsilon$ word. By the definition which we reuse from~\cite{smolkaHofmann2016}, the grammar in CNF has no epsilon rules, but we still need to consider the case when the empty word is derivable in the grammar. We postpone this consideration to the last step. Only one of the following statements is true: $\varepsilon \in L(G) \textit{ and } \varepsilon \in L(\textit{dfa})$ or $\neg \varepsilon \in L(G) \textit{ or } \neg \varepsilon \in L(\textit{dfa})$. So, we should just check emptiness of languages as a separated case.

\end{enumerate}


\subsection{Trivial Cases}

First, we consider the case when the number of the DFA states is zero.
In this case, we immediately derive a contradiction.
By definition, any DFA has an initial state.
It means that there is at least one state, which contradicts the assumption that the number of states is zero.

It is worth to mention, that in the proof~\cite{beigelproof} cases when the empty word is derivable in the grammar or a DFA specifies the empty language are discarded as trivial.
It is assumed that one can carry out themselves the proof for these cases.
In our proof, we include the trivial cases in the corresponding theorems.

\subsection{Regular Languages and Automata}

In this section, we describe definitions of DFA and DFA with exactly one final state, we also present the function that converts any DFA to a set of DFAs with one final state and lemma that states this split in some sense preserves the language specified.

We assume that a regular language is described by a DFA. We do not impose any restrictions on the type of input symbols and the number of states in DFA. Thus, the DFA is a 5-tuple: (1) a type of states, (2) a type of input symbols, (3) a start state, (4) a transition function, and (5) a list of final states.

Next, we define a function that evaluates the finish state of the automaton if it starts from the state $s$ and receives a word $w$.

We say that the automaton accepts a word $w$ being in state $s$ if the function $(\textit{final\_state} \ s \ w)$ returns a final state.
Finally, we say that an automaton accepts a word $w$, if the DFA starts from the initial state and stops in a final state.


The definition of the  DFA with exactly one final state differs from the definition of an ordinary DFA in that the list of final states is replaced by one final state.
Related definitions such as \textit{accepts} and \textit{dfa\_language} are slightly modified.

We define functions $\textit{s\_accepts}$ and $\textit{s\_dfa\_language}$ for DFA with one final state in the same fashion.
In the function $\textit{s\_accepts}$, it is enough to check for equality the state in which the automaton stopped with the finite state. Function $\textit{s\_dfa\_language}$ is the same as  $\textit{dfa\_language}$ except for that the function for a DFA with one final state should use $\textit{s\_accepts}$ instead of $\textit{accepts}$.

Now we can define a function that converts an ordinary DFA into a set of DFAs with exactly one final state.
Let $d$ be a DFA. Then the list of its final states is known.
For each such state, one can construct a copy of the original DFA, but with one selected final state.

As a result prove the theorem that the function of splitting preserves the language.

\begin{theorem}
  Let $\textbf{dfa}$ be an arbitrary DFA and $\textbf{w}$ be a word. Then the fact that $\textbf{dfa}$ accepts $\textbf{w}$ implies that there exists a single-state DFA $\textbf{s\_dfa}$, such that $\textbf{s\_dfa} \in \textbf{split\_dfa(dfa)}$ and $\textbf{s\_dfa}$ accepts $\textbf{w}$. And vice versa, for any $\textbf{s\_dfa} \in \textbf{split\_dfa(dfa)}$ the fact that $\textbf{s\_dfa}$ accepts a word $\textbf{w}$ implies that $\textbf{dfa}$ also accepts $\textbf{w}$.
\end{theorem}

\subsection{Chomsky Induction}
\label{sec:chomsky-induction}

Many statements about properties of words in a language can be proved by induction over derivation structure.
Although a one can get a phrase as an intermediate step of derivation, DFA only works on words, so we can not simply apply induction over the derivation structure. To tackle this problem, we created a custom induction principle for grammars in CNF.

The current definition of derivability does not imply the ability to ``reverse'' the derivation back. That is, nothing about the rules of the grammar or properties of derivation follows from the fact that a phrase $w$ is derived from a nonterminal $A$ in a grammar $G$. Because of this, we introduce an additional assumption on derivations that is similar to the syntactic analysis of words.
Namely, we assume that if the phrase $w$ is derived from the nonterminal $A$ in grammar $G$, then either there is a rule $A \to w \in G$ or there is a rule $A \to rhs \in G$ and $w$ is derivable from $rhs$.

Any word derivable from a nonterminal $A$ in the grammar in CNF is either a solitary terminal or can be split into two parts, each of which is derived from nonterminals $B$ and $C$, when the derivation starts with the rule $A \to B C$.
Note that if we naively take a step back, we can get a nonterminal which derives some substring in the middle of the word.
Such a situation does not make any sense for DFA.

By using induction, we always deal with subtrees that describe a substring of the word.

To put it more formally:
\begin{lemma} \label{lemma:chomskyind1}
Let $\textbf{G}$ be a grammar in CNF. Consider an arbitrary nonterminal $\textbf{N} \in \textbf{G}$ and phrase which consists only of terminals $\textbf{w}$.
If $\textbf{w}$ is derivable from $\boldsymbol{N}$ and $|\textbf{w}| \ge 2$, then there exists two nonterminals $\boldsymbol{N_1}, \boldsymbol{N_2}$ and two phrases $\boldsymbol{w_1, w_2}$ such that: $\boldsymbol{N \to N_1 N_2 \in G}$, $\boldsymbol{der(G, N_1, w_1)}$, $\boldsymbol{der(G, N_2, w_2)}$, $|\boldsymbol{w_1}| \ge 1$, $|\boldsymbol{w_2}| \ge 1$ and $\boldsymbol{w_1} \mathbin{++} \boldsymbol{w_2} = \textbf{w}$.
\end{lemma}

\begin{lemma}
	Let $\boldsymbol{G}$ be a grammar in CNF. And $\boldsymbol{P}$ be a predicate on nonterminals and phrases (i.e. $\textbf{P}: \textbf{var} \to \textbf{phrase} \to \textbf{Prop}$).
	Let's also assume that the following two hypotheses are satisfied:
	(1) for every terminal production (i.e. in the form $\boldsymbol{N} \to \boldsymbol{a}$) of grammar $\boldsymbol{G}$, $\boldsymbol{P}(\boldsymbol{r}, [\boldsymbol{r}])$ holds and (2) for every $\boldsymbol{N}, \boldsymbol{N_1}, \boldsymbol{N_2}$ such that: $\boldsymbol{N} \to \boldsymbol{N_1 N_2} \in \boldsymbol{G}$ and two phrases that consist only of terminals $\boldsymbol{w_1}, \boldsymbol{w_2}$, if $\boldsymbol{P}(\boldsymbol{N_1}, \boldsymbol{w_1})$, $\boldsymbol{P}(\boldsymbol{N_2}, \boldsymbol{w_2})$, $\boldsymbol{der}(\boldsymbol{G}, \boldsymbol{N_1}, \boldsymbol{w_1})$ and $\boldsymbol{der}(\boldsymbol{G}, \boldsymbol{N_2}, \boldsymbol{w_2})$ then $\boldsymbol{P}(\boldsymbol{N}, \boldsymbol{w_1} \mathbin{++} \boldsymbol{w_2})$.
	Then for any nonterminal $\boldsymbol{N}$ and any phrase consisting only of terminals $\boldsymbol{w}$, the fact that $\boldsymbol{w}$ is derivable from $\boldsymbol{N}$ implies $\boldsymbol{P}(\boldsymbol{N},\boldsymbol{w})$.
\end{lemma}

\subsection{Intersection of CFG and Automaton}

Since we already have lemmas about the transformation of a grammar to CNF and the transformation of a DFA to a DFA into a set of DFA's with exactly one accepting state, further we assume that we only deal with (1) DFA with exactly one final state---\textit{dfa} and (2) grammar in CNF---$G$. In this section, we describe the proof of the lemma that states that for any grammar in CNF and any automaton with exactly one state there is a grammar for an intersection of the languages.

\subsubsection{Construction of Intersection}

We present the adaptation of the algorithm given in~\cite{beigelproof}.

Let $G_{INT}$ be the grammar of intersection. In $G_{INT}$, nonterminals are presented as triples $(\textit{from} \times var \times to) $ where \textit{from} and $to$ are states of \textit{dfa}, and \textit{var} is a nonterminal of $G$.

Since $G$ is a grammar in CNF, it has only two types of productions: $(1)\ N \to a $ and $(2) \ N \to N_{1} N_{2}$, where $N, N_1, N_2$ are nonterminals and $a$ is a terminal.

For every production $N \to N_1 N_2$ in $G$ we generate a set of productions of the form $(\textit{from}, N, to) \to (\textit{from}, N_1,  m) (m, N_2, to)$ where: \textit{from}, $m$, $to$ enumerate all $\textit{dfa}$ states.

For every production of the form $N \to a$ we add a set of productions of the form $(\textit{from}, N, (\textit{dfa\_step}(\textit{from}, a))) \to a$ where $\textit{from}$ enumerates all $\textit{dfa}$ states and $\textit{dfa\_step (from, a)}$ is the state in which the $\textit{dfa}$ appears after receiving terminal $a$ in the state $\textit{from}$.

Next, we join the functions above to get a generic function that works for both types of productions.

Note that at this point we do not conduct any manipulations with the start nonterminal. Nevertheless, the hypothesis of the uniqueness of the final state of the DFA helps to define the start nonterminal of the grammar of intersection unambiguously. The start nonterminal for the intersection grammar is the following nonterminal: \textit{(start, S, final)} where: \textit{start}---the start state of DFA, \textit{S}---the start nonterminal of the initial grammar, and \textit{final}---the final state of DFA. Without the assumption that the DFA has only one final state it is not clear how to unequivocally define the start nonterminal over the alphabet of triples.

\subsubsection{Correctness of Intersection}
\label{sec:correctintersection}

In this subsection, we present a high-level description of the proof of correctness of the intersection function.

In the interest of clarity of exposition, we skip some auxiliary lemmas and facts like that we can get the initial grammar from the grammar of intersection by projecting the triples back to the corresponding terminals/nonterminals. Also note that grammar remains in CNF after the conversion, since the transformation of rules does not change the structure of them, but only replaces their terminals and nonterminals with attributed ones.

Next, we prove the following lemmas. First, the fact that a word can be derived in the initial grammar and is accepted by \textit{s\_dfa} implies it can be derived in the grammar of the intersection. And the other way around, the fact that a word can be derived in the grammar of the intersection implies that it is derived in the initial grammar and is accepted by \textit{s\_dfa}.

Let $G$ be a grammar in CNF. In order to use Chomsky Induction, we also assume that syntactic analysis is possible.

\begin{theorem}
    Let $ \textbf{s\_dfa} $ be an arbitrary DFA, let $\textbf{r}$ be a nonterminal of grammar $\textbf{G}$, let $ \textbf{from} $ and $ \textbf{to} $ be two states of the DFA. We also pick an arbitrary word---$\textbf{w}$. If it is possible to derive $\textbf{w}$ from $\textbf{r}$ and the $ \textbf{s\_dfa} $ starting from the state $ \textbf{from} $ finishes in the state $ \textbf{to} $ after consuming the word $\textbf{w}$, then the word $\textbf{w}$ is also derivable in grammar
    $ \textbf{(convert\_rules~G~next}) $ from the nonterminal $(\textbf{from}, \textbf{r}, \textbf{to})$.
\end{theorem}

On the other side, now we need to prove the theorems of the form  ``if it is derivable in the grammar of triples, then it is accepted by the automaton and is derivable in the initial grammar''.

We start with the DFA.

\begin{theorem}
	Let $ \textbf{from} $ and $ \textbf{to} $ be states of the automaton, $ \textbf{var} $ be an arbitrary nonterminal of $\textbf{G}$. We prove that if a word $\textbf{w}$ is derived from the nonterminal $ (\textbf{from}, \textbf{var}, \textbf{to}) $ in the grammar $ \textbf{(convert\_rules~G)} $, then the automaton starting from the state $ \textbf{from} $ accepts the word $\textbf{w}$ and stops in the state $\textbf{to} $.
\end{theorem}

Next, we prove the similar theorem for the grammar.

\begin{theorem}
	Let $ \textbf{from} $ and $ \textbf{to} $ be the states of the automaton, let $ \textbf{var} $ be an arbitrary nonterminal of grammar \textbf{G}. We prove that if a word $\textbf{w}$ is derivable from the nonterminal $ (\textbf{from}, \textbf{var}, \textbf{to}) $ in the grammar $ \textbf{(convert\_rules~G)}$, then $\textbf{w}$ is also derivable in the grammar $\textbf{G}$ from the nonterminal $ \textbf{var} $.
\end{theorem}

In the end, one needs to combine both theorems to get a full equivalence. By this, the correctness of the intersection is proved.

\subsection{Union of Languages}

During the previous step, we constructed a list of context-free grammars. In this section, we provide a function which constructs a grammar for the union of the languages.

First, we need to make sure the sets of nonterminals for each of the grammars under consideration have empty intersections. To achieve this, we label nonterminals. Each grammar of the union receives a unique ID number and all nonterminals within one grammar will have the same ID as the grammar. In addition, it is necessary to introduce a new start nonterminal of the union.

The function that constructs the union grammar takes a list of grammars, then, it (1) splits the list into head [$h$] and tail [$tl$], (2) labels [\textit{length \ tl}] to $h$, (3) adds a new rule from the start nonterminal of the union to the start nonterminal of the grammar [$h$], finally (4) the function is recursively called on the tail [$tl$] of the list.

\subsubsection{Proof of Languages Equivalence}

We prove that the function \textit{grammar\_union} constructs a correct grammar of the union language. Namely, we prove the following theorem.

\begin{theorem} \label{theorem-correct-union}
    Let \textbf{grammars} be a sequence of pairs of starting nonterminals and grammars. Then for any word $\textbf{w}$, the fact that $\textbf{w}$ belongs to the language of the union is equivalent to the fact that there exists a grammar $\textbf{(st,gr)} \in \textbf{grammars}$ such that $\textbf{w}$ belongs to the language generated by $\textbf{(st,gr)}$.
\end{theorem}


\subsection{Putting All Parts Together}

Now we can put all previously described lemmas together to prove the main statement of this paper.

\begin{figure}[h]
  \begin{Verbatim}[commandchars=\\\{\}]
  \PY{k+kn}{Theorem} \PY{n}{grammar\PYZus{}of\PYZus{}intersection\PYZus{}exists}\PY{o}{:}
    \PY{k}{exists}
      \PY{o}{(}\PY{n}{NewNonterminal}\PY{o}{:} \PY{k+kt}{Type}\PY{o}{)}
      \PY{o}{(}\PY{n}{IntersectionGrammar}\PY{o}{:} \PY{o}{@}\PY{n}{grammar} \PY{n}{Terminal} \PY{n}{NewNonterminal}\PY{o}{)} \PY{n}{St}\PY{o}{,}
    \PY{k}{forall} \PY{n}{word}\PY{o}{,}
       \PY{n}{dfa\PYZus{}language} \PY{n}{dfa} \PY{n}{word} \PY{o}{/\PYZbs{}} \PY{n}{language} \PY{n}{G} \PY{n}{S} \PY{o}{(}\PY{n}{to\PYZus{}phrase} \PY{n}{word}\PY{o}{)} \PY{o}{\PYZlt{}\PYZhy{}\PYZgt{}}
       \PY{n}{language} \PY{n}{IntersectionGrammar} \PY{n}{St} \PY{o}{(}\PY{n}{to\PYZus{}phrase} \PY{n}{word}\PY{o}{).}
  \end{Verbatim}

\caption{Final theorem}
\label{lst:lang-eq}
\end{figure}

\begin{theorem}
    Let $\boldsymbol{Tt}$ and $\boldsymbol{Nt}$ be a decidable types. $\boldsymbol{Tt}$ and $\boldsymbol{Nt}$ is types of terminals and nonterminals correspondingly.
    If there exists a bijection from $\boldsymbol{Nt}$ to $\mathbb{N}$ and syntactic analysis in the sense of definition is possible, then for any DFA \textbf{\textit{dfa}} that define language over $\boldsymbol{Tt}$ and any context-free grammar $\boldsymbol{G}$, there exists the context-free grammar $\boldsymbol{G_{INT}}$, such that $\boldsymbol{L}(\boldsymbol{G_{INT}}) = \boldsymbol{L}(\boldsymbol{G}) \cap \boldsymbol{L}(\textbf{dfa})$.
\end{theorem}


\section{Related Works}
\label{sec:rel-work}

There is a large number of contributions in the mechanization of different parts of formal languages theory and certified implementations of parsing algorithms and algorithms for graph database querying.
These works use various tools, such as Coq, Agda, HOL4, and are aimed at different problems such as the theory mechanization or executable algorithm certification.
We discuss only a small part which is close enough to the scope of this work.

\subsection{Formal Language Theory in Coq}

The massive amount of work was done by Ruy de Queiroz who formalized different parts of formal language theory, such as pumping lemma~\cite{ramos2015formalizationPumping,ramos2018SomeApp}, context-free grammar simplification~\cite{ramos2015formalization} and closure properties~\cite{ramos2015formalizationClosure} in Coq.
The work on closure properties contains mechanization of such properties as closure under union, Kleene star, but it does not contain mechanization of the intersection with a regular language.
All these results are summarized in~\cite{ramos2016formalization}.

Gert Smolka et al. also provide a large number of contributions on regular and context-free languages formalization in Coq~\cite{smolka2017regular,smolka2013regular,kaiser2012constructive,smolkaHofmann2016}.
The paper~\cite{smolkaHofmann2016} describes the certified transformation of an arbitrary context-free grammar to the Chomsky normal form which is required for our proof of the Bar-Hillel theorem.
Initially, we hoped to reuse these both parts because the Bar-Hillel theorem is about both context-free and regular languages, and it was the reason to choose results of Gert Smolka as the base for our work.
But the works on regular and on context-free languages are independent, and we are faced with the problems of reusing and integration, so in the current proof, we use only results on context-free languages.

\subsection{Formal Language Theory in Other Languages}

In the parallel with works in Coq there exist works on formal languages mechanization in other languages and tools such as Agda \cite{firsov2016cfl} or HOL4 \cite{barthwal2010formalisation}.

Firstly, there are works of Denis Firsov who implements some parts of the formal language theory and parsing algorithms in Agda.
In particular, Firsov implements CYK parsing algorithm~\cite{firsov2014certified,firsov2016cfl} and Chomsky Normal Form~\cite{firsov2015certified}, and some other results on regular languages~\cite{10.1007/978-3-319-03545-1_7}.

There are also works on the formal language theory mechanization in HOL-4~\cite{1885-16399,barthwal2010formalisation,10.1007/978-3-642-13824-9_11} by Aditi Barthwal and Michael Norrish.
This work contains basic definitions and a big number of theoretical results, such as Chomsky normal form and Greibach normal form for context-free grammars.
As an application of the mechanized theory authors, provide certified implementation of the SLR parsing algorithm~\cite{10.1007/978-3-642-00590-9_12}.

\section{Conclusion}
\label{sec:conclusion}

We present mechanized in Coq proof of the Bar-Hillel theorem, the fundamental theorem on the closure of context-free languages under intersection with the regular set.
By this, we increase mechanized part of formal language theory and provide a base for reasoning about many applicative algorithms which are based on language intersection.
We generalize the results of Gert Smolka and Jana Hofmann: the definition of the terminal and nonterminal alphabets in context-free grammar were made generic, and all related definitions and theorems were adjusted to work with the updated definition.
It makes previously existing results more flexible and eases reusing.
All results are published at GitHub and are equipped with automatically generated documentation.

The first open question is the integration of our results with other results on formal languages theory mechanization in Coq.
There are two independent sets of results in this area: works of Ruy de Queiroz and works of Gert Smolka.
We use part of Smolka's results in our work, but even here we do not use existing results on regular languages.
We believe that theory mechanization should be unified and results should be generalized.
We think that these and other related questions should be discussed in the community.

One direction for future research is mechanization of practical algorithms which are just implementation of the Bar-Hillel theorem.
For example, context-free path querying algorithm, based on CYK~\cite{hellingsPathQuerying,zhang2016context} or even on GLL~\cite{scott2010gll} parsing algorithm~\cite{grigorev2016context}.
Final target here is the certified algorithm for context-free constrained path querying for graph databases.

Another direction is mechanization of other problems on language intersection which can be useful for applications.
For example, the intersection of two context-free grammars one of which describes finite language~\cite{nederhof2002parsing,nederhof2004language}.
It may be useful for compressed data processing~\cite{Lohrey2012AlgorithmicsOS} or speech recognition~\cite{Nederhof:2002:PNC:1073083.1073104,NEDERHOF2004172}.
And we believe all these works should share the common base of mechanized theoretical results.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
 \bibliographystyle{splncs04}
 \begin{thebibliography}{10}
 \providecommand{\url}[1]{\texttt{#1}}
 \providecommand{\urlprefix}{URL }
 \providecommand{\doi}[1]{https://doi.org/#1}

 \bibitem{ABITEBOUL1999428}
 Abiteboul, S., Vianu, V.: {Regular Path Queries with Constraints}. Journal of
   Computer and System Sciences  \textbf{58}(3),  428 -- 452 (1999),
   \url{http://www.sciencedirect.com/science/article/pii/S0022000099916276}

 \bibitem{alkhateeb:tel-00293206}
 Alkhateeb, F.: {Querying RDF(S) with Regular Expressions}. Theses,
   {Universit{\'e} Joseph-Fourier - Grenoble I} (Jun 2008),
   \url{https://tel.archives-ouvertes.fr/tel-00293206}

 \bibitem{bar1961formal}
 Bar-Hillel, Y., Perles, M., Shamir, E.: {On Formal Properties of Simple Phrase
   Structure Grammars}. {Sprachtypologie und Universalienforschung}
   \textbf{14},  143--172 (1961)

 \bibitem{1885-16399}
 Barthwal, A.: {A formalisation of the theory of context-free languages in
   higher order logic}. Ph.D. thesis, {College of Engineering \& Computer
   Science, The Australian National University} (12 2010)

 \bibitem{10.1007/978-3-642-00590-9_12}
 Barthwal, A., Norrish, M.: {Verified, Executable Parsing}. In: Castagna, G.
   (ed.) Programming Languages and Systems. pp. 160--174. Springer Berlin
   Heidelberg, Berlin, Heidelberg (2009)

 \bibitem{barthwal2010formalisation}
 Barthwal, A., Norrish, M.: A formalisation of the normal forms of context-free
   grammars in {HOL4}. In: International Workshop on Computer Science Logic. pp.
   95--109. Springer (2010)

 \bibitem{10.1007/978-3-642-13824-9_11}
 Barthwal, A., Norrish, M.: {Mechanisation of PDA and Grammar Equivalence for
   Context-Free Languages}. In: Dawar, A., de~Queiroz, R. (eds.) {Logic,
   Language, Information and Computation}. pp. 125--135. Springer Berlin
   Heidelberg, Berlin, Heidelberg (2010)

 \bibitem{beigelproof}
 Beigel, R., Gasarch, W.: {A Proof that if $L = L_1 \cap L_2$ where $L_1$ is
   {CFL} and $L_2$ is Regular then $L$ is Context Free Which Does Not use
   PDA’s} \url{http://www.cs.umd.edu/~gasarch/BLOGPAPERS/cfg.pdf/}

 \bibitem{10.1007/3-540-63141-0_10}
 Bouajjani, A., Esparza, J., Maler, O.: {Reachability analysis of pushdown
   automata: Application to model-checking}. In: Mazurkiewicz, A., Winkowski, J.
   (eds.) CONCUR '97: Concurrency Theory. pp. 135--150. Springer Berlin
   Heidelberg, Berlin, Heidelberg (1997)

 \bibitem{smolka2013regular}
 Doczkal, C., Kaiser, J.O., Smolka, G.: {A Constructive Theory of Regular
   Languages in Coq}. In: {International Conference on Certified Programs and
   Proofs}. pp. 82--97. Springer (2013)

 \bibitem{smolka2017regular}
 Doczkal, C., Smolka, G.: {Regular Language Representations in the Constructive
   Type Theory of Coq}. Journal of Automated Reasoning  \textbf{61}(1),
   521--553 (Jun 2018), \url{https://doi.org/10.1007/s10817-018-9460-x}

 \bibitem{10.1007/11730637_17}
 Emmi, M., Majumdar, R.: {Decision Problems for the Verification of Real-Time
   Software}. In: Hespanha, J.P., Tiwari, A. (eds.) Hybrid Systems: Computation
   and Control. pp. 200--211. Springer Berlin Heidelberg, Berlin, Heidelberg
   (2006)

 \bibitem{firsov2016cfl}
 Firsov, D.: {Certification of Context-Free Grammar Algorithms}  (2016)

 \bibitem{10.1007/978-3-319-03545-1_7}
 Firsov, D., Uustalu, T.: {Certified Parsing of Regular Languages}. In:
   Gonthier, G., Norrish, M. (eds.) {Certified Programs and Proofs}. pp.
   98--113. Springer International Publishing, Cham (2013)

 \bibitem{firsov2014certified}
 Firsov, D., Uustalu, T.: {Certified CYK parsing of context-free languages}.
   Journal of Logical and Algebraic Methods in Programming  \textbf{83}(5-6),
   459--468 (2014)

 \bibitem{firsov2015certified}
 Firsov, D., Uustalu, T.: {Certified normalization of context-free grammars}.
   In: Proceedings of the 2015 Conference on Certified Programs and Proofs. pp.
   167--174. ACM (2015)

 \bibitem{grigorev2016context}
 Grigorev, S., Ragozina, A.: {Context-Free Path Querying with Structural
   Representation of Result}. arXiv preprint arXiv:1612.08872  (2016)

 \bibitem{Gross2015ParsingPA}
 Gross, J., Chlipala, A.: {Parsing Parses A Pearl of ( Dependently Typed )
   Programming and Proof} (2015)

 \bibitem{hellingsRelational}
 Hellings, J.: {Conjunctive Context-Free Path Queries}  (2014)

 \bibitem{hellingsPathQuerying}
 Hellings, J.: {Querying for Paths in Graphs using Context-Free Path Queries}.
   arXiv preprint arXiv:1502.02242  (2015)

 \bibitem{smolkaHofmann2016}
 Hofmann, J.: {Verified Algorithms for Context-Free Grammars in Coq} (2016)

 \bibitem{kaiser2012constructive}
 Kaiser, J.O.: {Constructive Formalization of Regular Languages}. Ph.D. thesis,
   Saarland University (2012)

 \bibitem{koschmieder2012regular}
 Koschmieder, A., Leser, U.: {Regular path queries on large graphs}. In:
   {International Conference on Scientific and Statistical Database Management}.
   pp. 177--194. Springer (2012)

 \bibitem{Lohrey2012AlgorithmicsOS}
 Lohrey, M.: {Algorithmics on SLP-compressed strings: A survey}. Groups
   Complexity Cryptology  \textbf{4},  241--299 (2012)

 \bibitem{lu2013incremental}
 Lu, Y., Shang, L., Xie, X., Xue, J.: {An incremental points-to analysis with
   CFL-reachability}. In: International Conference on Compiler Construction. pp.
   61--81. Springer (2013)

 \bibitem{MANETH201819}
 Maneth, S., Peternek, F.: {Grammar-based graph compression}. Information
   Systems  \textbf{76},  19 -- 45 (2018),
   \url{http://www.sciencedirect.com/science/article/pii/S0306437917301680}

 \bibitem{nederhof2002parsing}
 Nederhof, M.J., Satta, G.: {Parsing Non-Recursive Context-Free Grammars}. In:
   Proceedings of the 40th Annual Meeting on Association for Computational
   Linguistics. pp. 112--119. Association for Computational Linguistics (2002)

 \bibitem{Nederhof:2002:PNC:1073083.1073104}
 Nederhof, M.J., Satta, G.: {Parsing Non-recursive Context-free Grammars}. In:
   {Proceedings of the 40th Annual Meeting on Association for Computational
   Linguistics}. pp. 112--119. ACL '02, Association for Computational
   Linguistics, Stroudsburg, PA, USA (2002),
   \url{https://doi.org/10.3115/1073083.1073104}

 \bibitem{nederhof2004language}
 Nederhof, M.J., Satta, G.: {The language intersection problem for non-recursive
   context-free grammars}. Information and Computation  \textbf{192}(2),
   172--184 (2004)

 \bibitem{NEDERHOF2004172}
 Nederhof, M.J., Satta, G.: {The language intersection problem for non-recursive
   context-free grammars}. {Information and Computation}  \textbf{192}(2),  172
   -- 184 (2004),
   \url{http://www.sciencedirect.com/science/article/pii/S0890540104000562}

 \bibitem{pratikakis2006existential}
 Pratikakis, P., Foster, J.S., Hicks, M.: {Existential label flow inference via
   CFL reachability}. In: International Static Analysis Symposium. pp. 88--106.
   Springer (2006)

 \bibitem{ramos2015formalizationClosure}
 Ramos, M.V.M., de~Queiroz, R.J.G.B.: {Formalization of closure properties for
   context-free grammars}. CoRR  \textbf{abs/1506.03428} (2015),
   \url{http://arxiv.org/abs/1506.03428}

 \bibitem{ramos2015formalizationPumping}
 Ramos, M.V.M., de~Queiroz, R.J.G.B., Moreira, N., Almeida, J.C.B.:
   {Formalization of the pumping lemma for context-free languages}. CoRR
   \textbf{abs/1510.04748} (2015), \url{http://arxiv.org/abs/1510.04748}

 \bibitem{ramos2016formalization}
 Ramos, M.V.M., de~Queiroz, R.J., Moreira, N., Almeida, J.C.B.: {On the
   Formalization of Some Results of Context-Free Language Theory}. In:
   International Workshop on Logic, Language, Information, and Computation. pp.
   338--357. Springer (2016)

 \bibitem{ramos2018SomeApp}
 Ramos, M.V., Almeida, J.C.B., de~Queiroz, R.J., Moreira, N.: {Some Applications
   of the Formalization of the Pumping Lemma for Context-Free Languages}. In:
   Proceedings of the 13th Workshop on Logical and Semantic Frameworks with
   Applications. pp. 43--56 (2018)

 \bibitem{ramos2015formalization}
 Ramos, M.V., de~Queiroz, R.J.: {Formalization of simplification for
   context-free grammars}. arXiv preprint arXiv:1509.02032  (2015)

 \bibitem{rehof2001type}
 Rehof, J., F{\"a}hndrich, M.: {Type-base flow analysis: from polymorphic
   subtyping to CFL-reachability}. ACM SIGPLAN Notices  \textbf{36}(3),  54--66
   (2001)

 \bibitem{Reps:1995:PID:199448.199462}
 Reps, T., Horwitz, S., Sagiv, M.: {Precise Interprocedural Dataflow Analysis
   via Graph Reachability}. In: Proceedings of the 22Nd ACM SIGPLAN-SIGACT
   Symposium on Principles of Programming Languages. pp. 49--61. POPL '95, ACM,
   New York, NY, USA (1995), \url{http://doi.acm.org/10.1145/199448.199462}

 \bibitem{scott2010gll}
 Scott, E., Johnstone, A.: {GLL parsing}. Electronic Notes in Theoretical
   Computer Science  \textbf{253}(7),  177--189 (2010)

 \bibitem{SEKI1991191}
 Seki, H., Matsumura, T., Fujii, M., Kasami, T.: {On multiple context-free
   grammars}. Theoretical Computer Science  \textbf{88}(2),  191 -- 229 (1991),
   \url{http://www.sciencedirect.com/science/article/pii/030439759190374B}

 \bibitem{vardoulakis2010cfa2}
 Vardoulakis, D., Shivers, O.: {CFA2: A Context-free Approach to Control-flow
   Analysis}. In: Proceedings of the 19th European Conference on Programming
   Languages and Systems. pp. 570--589. ESOP'10, Springer-Verlag, Berlin,
   Heidelberg (2010), \url{http://dx.doi.org/10.1007/978-3-642-11957-6_30}

 \bibitem{Yan:2011:DCA:2001420.2001440}
 Yan, D., Xu, G., Rountev, A.: {Demand-driven Context-sensitive Alias Analysis
   for Java}. In: Proceedings of the 2011 International Symposium on Software
   Testing and Analysis. pp. 155--165. ISSTA '11, ACM, New York, NY, USA (2011),
   \url{http://doi.acm.org/10.1145/2001420.2001440}

 \bibitem{zhang2017context}
 Zhang, Q., Su, Z.: {Context-sensitive Data-dependence Analysis via Linear
   Conjunctive Language Reachability}. SIGPLAN Not.  \textbf{52}(1),  344--358
   (Jan 2017), \url{http://doi.acm.org/10.1145/3093333.3009848}

 \bibitem{zhang2016context}
 Zhang, X., Feng, Z., Wang, X., Rao, G., Wu, W.: {Context-Free Path Queries on
   RDF Graphs}. In: Groth, P., Simperl, E., Gray, A., Sabou, M., Kr{\"o}tzsch,
   M., Lecue, F., Fl{\"o}ck, F., Gil, Y. (eds.) The Semantic Web -- ISWC 2016.
   pp. 632--648. Springer International Publishing, Cham (2016)

 \end{thebibliography}

\appendix

\section {Coq Listing}
This listing contains main theorems and definitions from our work.

\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{Inductive} \PY{n}{ter} \PY{o}{:} \PY{k+kt}{Type} \PY{o}{:=} \PY{o}{|} \PY{n}{T} \PY{o}{:} \PY{n}{Tt} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{ter}\PY{o}{.}
\PY{k+kn}{Inductive} \PY{n}{var} \PY{o}{:} \PY{k+kt}{Type} \PY{o}{:=} \PY{o}{|} \PY{n}{V} \PY{o}{:} \PY{n}{Vt} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{var}\PY{o}{.}

\PY{k+kn}{Lemma} \PY{n}{language\PYZus{}normal\PYZus{}form} \PY{o}{(}\PY{n}{G}\PY{o}{:}\PY{n}{grammar}\PY{o}{)} \PY{o}{(}\PY{n}{A}\PY{o}{:} \PY{n}{var}\PY{o}{)} \PY{o}{(}\PY{n}{u}\PY{o}{:} \PY{n}{word}\PY{o}{):}
  \PY{n}{u} \PY{o}{\PYZlt{}\PYZgt{}} \PY{n+nb+bp}{[]} \PY{o}{\PYZhy{}\PYZgt{}} \PY{o}{(}\PY{n}{language} \PY{n}{G} \PY{n}{A} \PY{n}{u} \PY{o}{\PYZlt{}\PYZhy{}\PYZgt{}} \PY{n}{language} \PY{o}{(}\PY{n}{normalize} \PY{n}{G}\PY{o}{)} \PY{n}{A} \PY{n}{u}\PY{o}{).}

\PY{k+kn}{Inductive} \PY{n}{symbol} \PY{o}{:} \PY{k+kt}{Type} \PY{o}{:=}
  \PY{o}{|} \PY{n}{Ts} \PY{o}{:} \PY{n}{ter} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{symbol}
  \PY{o}{|} \PY{n}{Vs} \PY{o}{:} \PY{n}{var} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{symbol}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{word} \PY{o}{:=} \PY{k+kt}{list} \PY{n}{ter}\PY{o}{.}
\PY{k+kn}{Definition} \PY{n}{phrase} \PY{o}{:=} \PY{k+kt}{list} \PY{n}{symbol}\PY{o}{.}
\PY{k+kn}{Inductive} \PY{n}{rule} \PY{o}{:} \PY{k+kt}{Type} \PY{o}{:=} \PY{o}{|} \PY{n}{R} \PY{o}{:} \PY{n}{var} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{phrase} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{rule}\PY{o}{.}
\PY{k+kn}{Definition} \PY{n}{grammar} \PY{o}{:=} \PY{k+kt}{list} \PY{n}{rule}\PY{o}{.}

\PY{k+kn}{Inductive} \PY{n}{der} \PY{o}{(}\PY{n}{G} \PY{o}{:} \PY{n}{grammar}\PY{o}{)} \PY{o}{(}\PY{n}{A} \PY{o}{:} \PY{n}{var}\PY{o}{)} \PY{o}{:} \PY{n}{phrase} \PY{o}{\PYZhy{}\PYZgt{}} \PY{k+kt}{Prop} \PY{o}{:=}
  \PY{o}{|} \PY{n}{vDer} \PY{o}{:} \PY{n}{der} \PY{n}{G} \PY{n}{A} \PY{o}{[}\PY{n}{Vs} \PY{n}{A}\PY{o}{]}
  \PY{o}{|} \PY{n}{rDer} \PY{n}{l} \PY{o}{:} \PY{o}{(}\PY{n}{R} \PY{n}{A} \PY{n}{l}\PY{o}{)} \PY{n}{el} \PY{n}{G} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{der} \PY{n}{G} \PY{n}{A} \PY{n}{l}
  \PY{o}{|} \PY{n}{replN} \PY{n}{B} \PY{n}{u} \PY{n}{w} \PY{n}{v} \PY{o}{:}
    \PY{n}{der} \PY{n}{G} \PY{n}{A} \PY{o}{(}\PY{n}{u} \PY{o}{++} \PY{o}{[}\PY{n}{Vs} \PY{n}{B}\PY{o}{]} \PY{o}{++} \PY{n}{w}\PY{o}{)} \PY{o}{\PYZhy{}\PYZgt{}}
    \PY{n}{der} \PY{n}{G} \PY{n}{B} \PY{n}{v} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{der} \PY{n}{G} \PY{n}{A} \PY{o}{(}\PY{n}{u} \PY{o}{++} \PY{n}{v} \PY{o}{++} \PY{n}{w}\PY{o}{).}

\PY{k+kn}{Definition} \PY{n}{language} \PY{o}{(}\PY{n}{G} \PY{o}{:} \PY{n}{grammar}\PY{o}{)} \PY{o}{(}\PY{n}{A} \PY{o}{:} \PY{n}{var}\PY{o}{)} \PY{o}{(}\PY{n}{w} \PY{o}{:} \PY{n}{phrase}\PY{o}{)} \PY{o}{:=}
  \PY{n}{der} \PY{n}{G} \PY{n}{A} \PY{n}{w} \PY{o}{/\PYZbs{}} \PY{n}{terminal} \PY{n}{w}\PY{o}{.}

\PY{k+kn}{Context} \PY{o}{\PYZob{}}\PY{n}{State} \PY{n}{T}\PY{o}{:} \PY{k+kt}{Type}\PY{o}{\PYZcb{}.}

\PY{k+kn}{Record} \PY{n}{dfa}\PY{o}{:} \PY{k+kt}{Type} \PY{o}{:=}
  \PY{n}{mkDfa} \PY{o}{\PYZob{}}
    \PY{n}{start}\PY{o}{:} \PY{n}{State}\PY{o}{;}
    \PY{n}{final}\PY{o}{:} \PY{k+kt}{list} \PY{n}{State}\PY{o}{;}
    \PY{n}{next}\PY{o}{:} \PY{n}{State} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{ter} \PY{n}{T} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{State}\PY{o}{;}
  \PY{o}{\PYZcb{}.}

\PY{k+kn}{Fixpoint} \PY{n}{final\PYZus{}state} \PY{o}{(}\PY{n}{next\PYZus{}d}\PY{o}{:} \PY{n}{dfa\PYZus{}rule}\PY{o}{)} \PY{o}{(}\PY{n}{s}\PY{o}{:} \PY{n}{State}\PY{o}{)} \PY{o}{(}\PY{n}{w}\PY{o}{:} \PY{n}{word}\PY{o}{):} \PY{n}{State} \PY{o}{:=}
  \PY{k}{match} \PY{n}{w} \PY{k}{with}
  \PY{o}{|} \PY{n}{nil} \PY{o}{=\PYZgt{}} \PY{n}{s}
  \PY{o}{|} \PY{n}{h} \PY{o}{::} \PY{n}{t} \PY{o}{=\PYZgt{}} \PY{n}{final\PYZus{}state} \PY{n}{next\PYZus{}d} \PY{o}{(}\PY{n}{next\PYZus{}d} \PY{n}{s} \PY{n}{h}\PY{o}{)} \PY{n}{t}
  \PY{k}{end}\PY{o}{.}

\PY{k+kn}{Record} \PY{n}{s\PYZus{}dfa} \PY{o}{:} \PY{k+kt}{Type} \PY{o}{:=}
  \PY{n}{s\PYZus{}mkDfa} \PY{o}{\PYZob{}}
    \PY{n}{s\PYZus{}start}\PY{o}{:} \PY{n}{State}\PY{o}{;}
    \PY{n}{s\PYZus{}final}\PY{o}{:} \PY{n}{State}\PY{o}{;}
    \PY{n}{s\PYZus{}next}\PY{o}{:} \PY{n}{State} \PY{o}{\PYZhy{}\PYZgt{}} \PY{o}{(@}\PY{n}{ter} \PY{n}{T}\PY{o}{)} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{State}\PY{o}{;}
  \PY{o}{\PYZcb{}.}

\PY{k+kn}{Fixpoint} \PY{n}{split\PYZus{}dfa\PYZus{}list} \PY{o}{(}\PY{n}{st\PYZus{}d} \PY{o}{:} \PY{n}{State}\PY{o}{)} \PY{o}{(}\PY{n}{next\PYZus{}d} \PY{o}{:} \PY{n}{dfa\PYZus{}rule}\PY{o}{)}
                        \PY{o}{(}\PY{n}{f\PYZus{}list} \PY{o}{:} \PY{k+kt}{list} \PY{n}{State}\PY{o}{):} \PY{k+kt}{list} \PY{o}{(}\PY{n}{s\PYZus{}dfa}\PY{o}{)} \PY{o}{:=}
  \PY{k}{match} \PY{n}{f\PYZus{}list} \PY{k}{with}
  \PY{o}{|} \PY{n}{nil} \PY{o}{=\PYZgt{}} \PY{n}{nil}
  \PY{o}{|} \PY{n}{h} \PY{o}{::} \PY{n}{t} \PY{o}{=\PYZgt{}} \PY{o}{(}\PY{n}{s\PYZus{}mkDfa} \PY{n}{st\PYZus{}d} \PY{n}{h} \PY{n}{next\PYZus{}d}\PY{o}{)} \PY{o}{::} \PY{n}{split\PYZus{}dfa\PYZus{}list} \PY{n}{st\PYZus{}d} \PY{n}{next\PYZus{}d} \PY{n}{t}
  \PY{k}{end}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{split\PYZus{}dfa} \PY{o}{(}\PY{n}{d}\PY{o}{:} \PY{n}{dfa}\PY{o}{)} \PY{o}{:=}
  \PY{n}{split\PYZus{}dfa\PYZus{}list} \PY{o}{(}\PY{n}{start} \PY{n}{d}\PY{o}{)} \PY{o}{(}\PY{n}{next} \PY{n}{d}\PY{o}{)} \PY{o}{(}\PY{n}{final} \PY{n}{d}\PY{o}{).}

\PY{k+kn}{Lemma} \PY{n}{correct\PYZus{}split}\PY{o}{:}
  \PY{k}{forall} \PY{n}{dfa} \PY{n}{w}\PY{o}{,}
    \PY{n}{dfa\PYZus{}language} \PY{n}{dfa} \PY{n}{w} \PY{o}{\PYZlt{}\PYZhy{}\PYZgt{}}
    \PY{k}{exists} \PY{n}{sdfa}\PY{o}{,} \PY{n}{In} \PY{n}{sdfa} \PY{o}{(}\PY{n}{split\PYZus{}dfa} \PY{n}{dfa}\PY{o}{)} \PY{o}{/\PYZbs{}} \PY{n}{s\PYZus{}dfa\PYZus{}language} \PY{n}{sdfa} \PY{n}{w}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{syntactic\PYZus{}analysis\PYZus{}is\PYZus{}possible} \PY{o}{:=}
  \PY{k}{forall} \PY{o}{(}\PY{n}{G} \PY{o}{:} \PY{n}{grammar}\PY{o}{)} \PY{o}{(}\PY{n}{A} \PY{o}{:} \PY{n}{var}\PY{o}{)} \PY{o}{(}\PY{n}{w} \PY{o}{:} \PY{n}{phrase}\PY{o}{),}
  \PY{n}{der} \PY{n}{G} \PY{n}{A} \PY{n}{w} \PY{o}{\PYZhy{}\PYZgt{}} \PY{o}{(}\PY{n}{R} \PY{n}{A} \PY{n}{w} \PY{err}{\PYZbs{}}\PY{k}{in} \PY{n}{G}\PY{o}{)} \PY{o}{\PYZbs{}/} \PY{o}{(}\PY{k}{exists} \PY{n}{rhs}\PY{o}{,} \PY{n}{R} \PY{n}{A} \PY{n}{rhs} \PY{err}{\PYZbs{}}\PY{k}{in} \PY{n}{G} \PY{o}{/\PYZbs{}} \PY{n}{derf} \PY{n}{G} \PY{n}{rhs} \PY{n}{w}\PY{o}{).}

\PY{k+kn}{Definition} \PY{n}{convert\PYZus{}nonterm\PYZus{}rule\PYZus{}2} \PY{o}{(}\PY{n}{r} \PY{n}{r1} \PY{n}{r2}\PY{o}{:} \PY{o}{\PYZus{})} \PY{o}{(}\PY{n}{state1} \PY{n}{state2} \PY{o}{:} \PY{o}{\PYZus{})} \PY{o}{:=}
  \PY{n}{map} \PY{o}{(}\PY{k}{fun} \PY{n}{s3} \PY{o}{=\PYZgt{}} \PY{n}{R} \PY{o}{(}\PY{n}{V} \PY{o}{(}\PY{n}{s1}\PY{o}{,} \PY{n}{r}\PY{o}{,} \PY{n}{s3}\PY{o}{))}
                   \PY{o}{[}\PY{n}{Vs} \PY{o}{(}\PY{n}{V} \PY{o}{(}\PY{n}{s1}\PY{o}{,} \PY{n}{r1}\PY{o}{,} \PY{n}{s2}\PY{o}{));} \PY{n}{Vs} \PY{o}{(}\PY{n}{V} \PY{o}{(}\PY{n}{s2}\PY{o}{,} \PY{n}{r2}\PY{o}{,} \PY{n}{s3}\PY{o}{))])}
      \PY{n}{list\PYZus{}of\PYZus{}states}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{convert\PYZus{}nonterm\PYZus{}rule\PYZus{}1} \PY{o}{(}\PY{n}{r} \PY{n}{r1} \PY{n}{r2}\PY{o}{:} \PY{o}{\PYZus{})} \PY{o}{(}\PY{n}{s1} \PY{o}{:} \PY{o}{\PYZus{})} \PY{o}{:=}
  \PY{n}{flat\PYZus{}map} \PY{o}{(}\PY{n}{convert\PYZus{}nonterm\PYZus{}rule\PYZus{}2} \PY{n}{r} \PY{n}{r1} \PY{n}{r2} \PY{n}{s1}\PY{o}{)} \PY{n}{list\PYZus{}of\PYZus{}states}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{convert\PYZus{}nonterm\PYZus{}rule} \PY{o}{(}\PY{n}{r} \PY{n}{r1} \PY{n}{r2}\PY{o}{:} \PY{o}{\PYZus{})} \PY{o}{:=}
  \PY{n}{flat\PYZus{}map} \PY{o}{(}\PY{n}{convert\PYZus{}nonterm\PYZus{}rule\PYZus{}1} \PY{n}{r} \PY{n}{r1} \PY{n}{r2}\PY{o}{)} \PY{n}{list\PYZus{}of\PYZus{}states}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{convert\PYZus{}terminal\PYZus{}rule}
  \PY{o}{(}\PY{n}{next}\PY{o}{:} \PY{o}{\PYZus{})} \PY{o}{(}\PY{n}{r}\PY{o}{:} \PY{o}{\PYZus{})} \PY{o}{(}\PY{n}{t}\PY{o}{:} \PY{o}{\PYZus{}):} \PY{k+kt}{list} \PY{n}{TripleRule} \PY{o}{:=}
  \PY{n}{map} \PY{o}{(}\PY{k}{fun} \PY{n}{s1} \PY{o}{=\PYZgt{}} \PY{n}{R} \PY{o}{(}\PY{n}{V} \PY{o}{(}\PY{n}{s1}\PY{o}{,} \PY{n}{r}\PY{o}{,} \PY{n}{next} \PY{n}{s1} \PY{n}{t}\PY{o}{))} \PY{o}{[}\PY{n}{Ts} \PY{n}{t}\PY{o}{])} \PY{n}{list\PYZus{}of\PYZus{}states}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{convert\PYZus{}rule} \PY{o}{(}\PY{n}{next}\PY{o}{:} \PY{o}{\PYZus{})} \PY{o}{(}\PY{n}{r}\PY{o}{:} \PY{o}{\PYZus{}} \PY{o}{)} \PY{o}{:=}
   \PY{k}{match} \PY{n}{r} \PY{k}{with}
   \PY{o}{|} \PY{n}{R} \PY{n}{r} \PY{o}{[}\PY{n}{Vs} \PY{n}{r1}\PY{o}{;} \PY{n}{Vs} \PY{n}{r2}\PY{o}{]} \PY{o}{=\PYZgt{}}
       \PY{n}{convert\PYZus{}nonterm\PYZus{}rule} \PY{n}{r} \PY{n}{r1} \PY{n}{r2}
   \PY{o}{|} \PY{n}{R} \PY{n}{r} \PY{o}{[}\PY{n}{Ts} \PY{n}{t}\PY{o}{]} \PY{o}{=\PYZgt{}}
       \PY{n}{convert\PYZus{}terminal\PYZus{}rule} \PY{n}{next} \PY{n}{r} \PY{n}{t}
   \PY{o}{|} \PY{o}{\PYZus{}}  \PY{o}{=\PYZgt{}} \PY{n+nb+bp}{[]}   \PY{c}{(* Never called *)}
   \PY{k}{end}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{convert\PYZus{}rules}
  \PY{o}{(}\PY{n}{rules}\PY{o}{:} \PY{k+kt}{list} \PY{n}{rule}\PY{o}{)} \PY{o}{(}\PY{n}{next}\PY{o}{:} \PY{o}{\PYZus{}):} \PY{k+kt}{list} \PY{n}{rule} \PY{o}{:=}
  \PY{n}{flat\PYZus{}map} \PY{o}{(}\PY{n}{convert\PYZus{}rule} \PY{n}{next}\PY{o}{)} \PY{n}{rules}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{convert\PYZus{}grammar} \PY{n}{grammar} \PY{n}{s\PYZus{}dfa} \PY{o}{:=}
  \PY{n}{convert\PYZus{}rules} \PY{n}{grammar} \PY{o}{(}\PY{n}{s\PYZus{}next} \PY{n}{s\PYZus{}dfa}\PY{o}{).}

\PY{k+kn}{Inductive} \PY{n}{labeled\PYZus{}Vt} \PY{o}{:} \PY{k+kt}{Type} \PY{o}{:=}
  \PY{o}{|} \PY{n}{start} \PY{o}{:} \PY{n}{labeled\PYZus{}Vt}
  \PY{o}{|} \PY{n}{lV} \PY{o}{:} \PY{k+kt}{nat} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{Vt} \PY{o}{\PYZhy{}\PYZgt{}} \PY{n}{labeled\PYZus{}Vt}\PY{o}{.}

\PY{k+kn}{Definition} \PY{n}{label\PYZus{}var} \PY{o}{(}\PY{n}{label}\PY{o}{:} \PY{k+kt}{nat}\PY{o}{)} \PY{o}{(}\PY{n}{v}\PY{o}{:} \PY{o}{@}\PY{n}{var} \PY{n}{Vt}\PY{o}{):} \PY{o}{@}\PY{n}{var} \PY{n}{labeled\PYZus{}Vt} \PY{o}{:=}
  \PY{n}{V} \PY{o}{(}\PY{n}{lV} \PY{n}{label} \PY{n}{v}\PY{o}{).}

\PY{k+kn}{Definition} \PY{n}{label\PYZus{}grammar\PYZus{}and\PYZus{}add\PYZus{}start\PYZus{}rule} \PY{n}{label} \PY{n}{grammar} \PY{o}{:=}
  \PY{k}{let} \PY{k}{\PYZsq{}}\PY{o}{(}\PY{n}{st}\PY{o}{,} \PY{n}{gr}\PY{o}{)} \PY{o}{:=} \PY{n}{grammar} \PY{k}{in}
  \PY{o}{(}\PY{n}{R} \PY{o}{(}\PY{n}{V} \PY{n}{start}\PY{o}{)} \PY{o}{[}\PY{n}{Vs} \PY{o}{(}\PY{n}{V} \PY{o}{(}\PY{n}{lV} \PY{n}{label} \PY{n}{st}\PY{o}{))])} \PY{o}{::} \PY{n}{label\PYZus{}grammar} \PY{n}{label} \PY{n}{gr}\PY{o}{.}

\PY{k+kn}{Fixpoint} \PY{n}{grammar\PYZus{}union} \PY{o}{(}\PY{n}{grammars} \PY{o}{:} \PY{n}{seq} \PY{o}{(@}\PY{n}{var} \PY{n}{Vt} \PY{o}{*} \PY{o}{(@}\PY{n}{grammar} \PY{n}{Tt} \PY{n}{Vt}\PY{o}{)))}
       \PY{o}{:} \PY{o}{@}\PY{n}{grammar} \PY{n}{Tt} \PY{n}{labeled\PYZus{}Vt} \PY{o}{:=}
  \PY{k}{match} \PY{n}{grammars} \PY{k}{with}
  \PY{o}{|} \PY{n+nb+bp}{[]} \PY{o}{=\PYZgt{}} \PY{n+nb+bp}{[]}
  \PY{o}{|}  \PY{o}{(}\PY{n}{g}\PY{o}{::}\PY{n}{t}\PY{o}{)} \PY{o}{=\PYZgt{}} \PY{n}{label\PYZus{}grammar\PYZus{}and\PYZus{}add\PYZus{}start\PYZus{}rule} \PY{o}{(}\PY{n}{length} \PY{n}{t}\PY{o}{)}
                                                \PY{n}{g} \PY{o}{++} \PY{o}{(}\PY{n}{grammar\PYZus{}union} \PY{n}{t}\PY{o}{)}
\PY{k}{end}\PY{o}{.}

\PY{k+kn}{Variable} \PY{n}{grammars}\PY{o}{:} \PY{n}{seq} \PY{o}{(}\PY{n}{var} \PY{o}{*} \PY{n}{grammar}\PY{o}{).}

\PY{k+kn}{Theorem} \PY{n}{correct\PYZus{}union}\PY{o}{:}
  \PY{k}{forall} \PY{n}{word}\PY{o}{,}
    \PY{n}{language} \PY{o}{(}\PY{n}{grammar\PYZus{}union} \PY{n}{grammars}\PY{o}{)}
    \PY{o}{(}\PY{n}{V} \PY{o}{(}\PY{n}{start} \PY{n}{Vt}\PY{o}{))} \PY{o}{(}\PY{n}{to\PYZus{}phrase} \PY{n}{word}\PY{o}{)} \PY{o}{\PYZlt{}\PYZhy{}\PYZgt{}}
  \PY{k}{exists} \PY{n}{s\PYZus{}l}\PY{o}{,}
    \PY{n}{language} \PY{o}{(}\PY{n}{snd} \PY{n}{s\PYZus{}l}\PY{o}{)} \PY{o}{(}\PY{n}{fst} \PY{n}{s\PYZus{}l}\PY{o}{)} \PY{o}{(}\PY{n}{to\PYZus{}phrase} \PY{n}{word}\PY{o}{)} \PY{o}{/\PYZbs{}} \PY{n}{In} \PY{n}{s\PYZus{}l} \PY{n}{grammars}\PY{o}{.}

\PY{k+kn}{Theorem} \PY{n}{grammar\PYZus{}of\PYZus{}intersection\PYZus{}exists}\PY{o}{:}
  \PY{k}{exists}
    \PY{o}{(}\PY{n}{NewNonterminal}\PY{o}{:} \PY{k+kt}{Type}\PY{o}{)}
    \PY{o}{(}\PY{n}{IntersectionGrammar}\PY{o}{:} \PY{o}{@}\PY{n}{grammar} \PY{n}{Terminal} \PY{n}{NewNonterminal}\PY{o}{)} \PY{n}{St}\PY{o}{,}
  \PY{k}{forall} \PY{n}{word}\PY{o}{,}
     \PY{n}{dfa\PYZus{}language} \PY{n}{dfa} \PY{n}{word} \PY{o}{/\PYZbs{}} \PY{n}{language} \PY{n}{G} \PY{n}{S} \PY{o}{(}\PY{n}{to\PYZus{}phrase} \PY{n}{word}\PY{o}{)} \PY{o}{\PYZlt{}\PYZhy{}\PYZgt{}}
     \PY{n}{language} \PY{n}{IntersectionGrammar} \PY{n}{St} \PY{o}{(}\PY{n}{to\PYZus{}phrase} \PY{n}{word}\PY{o}{).}
\end{Verbatim}

\end{document}
